
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GRNNLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_reward_set.csv
Model: GRNN_1x3_RNN_acceptreject_asymmetric_qp_no-punishment_2022_10_04_02_17_35_300308
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6928	Validation Loss: 0.6910
Epoch 500: 	Training Loss: 0.6376	Validation Loss: 0.6117
Epoch 1000: 	Training Loss: 0.6348	Validation Loss: 0.6119
Epoch 1500: 	Training Loss: 0.6514	Validation Loss: 0.6223
Epoch 2000: 	Training Loss: 0.6362	Validation Loss: 0.6061
Epoch 2500: 	Training Loss: 0.6579	Validation Loss: 0.6365
Epoch 3000: 	Training Loss: 0.6351	Validation Loss: 0.6031
Epoch 3500: 	Training Loss: 0.6339	Validation Loss: 0.6031
Epoch 4000: 	Training Loss: 0.6365	Validation Loss: 0.6035
Epoch 4500: 	Training Loss: 0.6336	Validation Loss: 0.6052
Epoch 5000: 	Training Loss: 0.6409	Validation Loss: 0.6114
Epoch 5500: 	Training Loss: 0.6341	Validation Loss: 0.6167
Epoch 6000: 	Training Loss: 0.6407	Validation Loss: 0.6050
Early stopping at epoch 6101
Best validation loss: 0.6001
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6986	Validation Loss: 0.6935
Epoch 500: 	Training Loss: 0.6184	Validation Loss: 0.6385
Epoch 1000: 	Training Loss: 0.6173	Validation Loss: 0.6370
Epoch 1500: 	Training Loss: 0.6168	Validation Loss: 0.6377
Epoch 2000: 	Training Loss: 0.6289	Validation Loss: 0.6489
Epoch 2500: 	Training Loss: 0.6370	Validation Loss: 0.6609
Epoch 3000: 	Training Loss: 0.6166	Validation Loss: 0.6386
Epoch 3500: 	Training Loss: 0.6163	Validation Loss: 0.6367
Early stopping at epoch 3724
Best validation loss: 0.6353
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6939	Validation Loss: 0.6926
Epoch 500: 	Training Loss: 0.6303	Validation Loss: 0.6142
Epoch 1000: 	Training Loss: 0.6257	Validation Loss: 0.6179
Epoch 1500: 	Training Loss: 0.6256	Validation Loss: 0.6181
Epoch 2000: 	Training Loss: 0.6279	Validation Loss: 0.6222
Epoch 2500: 	Training Loss: 0.6240	Validation Loss: 0.6217
Epoch 3000: 	Training Loss: 0.6237	Validation Loss: 0.6204
Early stopping at epoch 3009
Best validation loss: 0.6128
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6940	Validation Loss: 0.6918
Epoch 500: 	Training Loss: 0.6306	Validation Loss: 0.6157
Epoch 1000: 	Training Loss: 0.6309	Validation Loss: 0.6111
Epoch 1500: 	Training Loss: 0.6284	Validation Loss: 0.6140
Epoch 2000: 	Training Loss: 0.6276	Validation Loss: 0.6114
Epoch 2500: 	Training Loss: 0.6282	Validation Loss: 0.6155
Epoch 3000: 	Training Loss: 0.6277	Validation Loss: 0.6117
Epoch 3500: 	Training Loss: 0.6282	Validation Loss: 0.6101
Epoch 4000: 	Training Loss: 0.6299	Validation Loss: 0.6139
Epoch 4500: 	Training Loss: 0.6274	Validation Loss: 0.6129
Early stopping at epoch 4701
Best validation loss: 0.6074
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6971	Validation Loss: 0.6958
Epoch 500: 	Training Loss: 0.6202	Validation Loss: 0.6530
Epoch 1000: 	Training Loss: 0.6192	Validation Loss: 0.6552
Epoch 1500: 	Training Loss: 0.6177	Validation Loss: 0.6555
Epoch 2000: 	Training Loss: 0.6215	Validation Loss: 0.6550
Epoch 2500: 	Training Loss: 0.6166	Validation Loss: 0.6578
Epoch 3000: 	Training Loss: 0.6163	Validation Loss: 0.6594
Early stopping at epoch 3020
Best validation loss: 0.6506
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/GRNN_1x3_RNN_acceptreject_asymmetric_qp_no-punishment_2022_10_04_02_17_35_300308/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u12>
Subject: Job 126232898: <GRNNx3-2-0> in cluster <Janelia> Done

Job <GRNNx3-2-0> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Tue Oct  4 02:17:05 2022
Job was executed on host(s) <e10u12>, in queue <local>, as user <mohantas> in cluster <Janelia> at Tue Oct  4 02:17:05 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Tue Oct  4 02:17:05 2022
Terminated at Tue Oct  4 03:26:57 2022
Results reported at Tue Oct  4 03:26:57 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_rajagopalan.py --agent GRNN --num_reservoir 1 --reservoir_size 3 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric no --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   8267.78 sec.
    Max Memory :                                 250 MB
    Average Memory :                             232.34 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               15110.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                14
    Run time :                                   4194 sec.
    Turnaround time :                            4192 sec.

The output (if any) is above this job summary.


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GRNNLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_reward_set.csv
Model: GRNN_1x3_RNN_acceptreject_asymmetric_qp_no-punishment_2022_10_08_20_14_46_228687
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6890	Validation Loss: 0.6776
Epoch 500: 	Training Loss: 0.5996	Validation Loss: 0.5835
Epoch 1000: 	Training Loss: 0.5996	Validation Loss: 0.5853
Epoch 1500: 	Training Loss: 0.5995	Validation Loss: 0.5839
Epoch 2000: 	Training Loss: 0.5998	Validation Loss: 0.5852
Epoch 2500: 	Training Loss: 0.6005	Validation Loss: 0.5852
Early stopping at epoch 2833
Best validation loss: 0.5831
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6951	Validation Loss: 0.6916
Epoch 500: 	Training Loss: 0.5898	Validation Loss: 0.6266
Epoch 1000: 	Training Loss: 0.5897	Validation Loss: 0.6296
Epoch 1500: 	Training Loss: 0.5892	Validation Loss: 0.6382
Epoch 2000: 	Training Loss: 0.5902	Validation Loss: 0.6252
Epoch 2500: 	Training Loss: 0.5909	Validation Loss: 0.6244
Epoch 3000: 	Training Loss: 0.5887	Validation Loss: 0.6232
Epoch 3500: 	Training Loss: 0.5889	Validation Loss: 0.6265
Epoch 4000: 	Training Loss: 0.5893	Validation Loss: 0.6238
Epoch 4500: 	Training Loss: 0.5888	Validation Loss: 0.6234
Epoch 5000: 	Training Loss: 0.5895	Validation Loss: 0.6246
Epoch 5500: 	Training Loss: 0.5895	Validation Loss: 0.6316
Early stopping at epoch 5598
Best validation loss: 0.6221
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6943	Validation Loss: 0.6840
Epoch 500: 	Training Loss: 0.5887	Validation Loss: 0.6260
Epoch 1000: 	Training Loss: 0.5894	Validation Loss: 0.6303
Epoch 1500: 	Training Loss: 0.5916	Validation Loss: 0.6636
Epoch 2000: 	Training Loss: 0.5930	Validation Loss: 0.6277
Epoch 2500: 	Training Loss: 0.5899	Validation Loss: 0.6265
Epoch 3000: 	Training Loss: 0.5898	Validation Loss: 0.6274
Epoch 3500: 	Training Loss: 0.5926	Validation Loss: 0.6284
Epoch 4000: 	Training Loss: 0.5910	Validation Loss: 0.6284
Epoch 4500: 	Training Loss: 0.5905	Validation Loss: 0.6270
Epoch 5000: 	Training Loss: 0.5919	Validation Loss: 0.6323
Epoch 5500: 	Training Loss: 0.5898	Validation Loss: 0.6295
Epoch 6000: 	Training Loss: 0.5890	Validation Loss: 0.6248
Early stopping at epoch 6276
Best validation loss: 0.6232
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6896	Validation Loss: 0.6822
Epoch 500: 	Training Loss: 0.5931	Validation Loss: 0.6102
Epoch 1000: 	Training Loss: 0.5951	Validation Loss: 0.6140
Epoch 1500: 	Training Loss: 0.5945	Validation Loss: 0.6111
Epoch 2000: 	Training Loss: 0.5928	Validation Loss: 0.6100
Epoch 2500: 	Training Loss: 0.5933	Validation Loss: 0.6122
Epoch 3000: 	Training Loss: 0.5962	Validation Loss: 0.6183
Epoch 3500: 	Training Loss: 0.5925	Validation Loss: 0.6118
Early stopping at epoch 3932
Best validation loss: 0.6088
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6856	Validation Loss: 0.6695
Epoch 500: 	Training Loss: 0.5978	Validation Loss: 0.6009
Epoch 1000: 	Training Loss: 0.5988	Validation Loss: 0.5976
Epoch 1500: 	Training Loss: 0.5977	Validation Loss: 0.5969
Epoch 2000: 	Training Loss: 0.5980	Validation Loss: 0.5951
Epoch 2500: 	Training Loss: 0.5979	Validation Loss: 0.5946
Epoch 3000: 	Training Loss: 0.5987	Validation Loss: 0.5937
Epoch 3500: 	Training Loss: 0.5977	Validation Loss: 0.5940
Epoch 4000: 	Training Loss: 0.5979	Validation Loss: 0.5948
Epoch 4500: 	Training Loss: 0.5964	Validation Loss: 0.5963
Epoch 5000: 	Training Loss: 0.5973	Validation Loss: 0.5950
Early stopping at epoch 5422
Best validation loss: 0.5931
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/GRNN_1x3_RNN_acceptreject_asymmetric_qp_no-punishment_2022_10_08_20_14_46_228687/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u13>
Subject: Job 126381324: <GRNNx3-2-0> in cluster <Janelia> Done

Job <GRNNx3-2-0> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Sat Oct  8 20:14:32 2022
Job was executed on host(s) <e10u13>, in queue <local>, as user <mohantas> in cluster <Janelia> at Sat Oct  8 20:14:33 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Sat Oct  8 20:14:33 2022
Terminated at Sun Oct  9 01:27:05 2022
Results reported at Sun Oct  9 01:27:05 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_mohanta.py --agent GRNN --num_reservoir 1 --reservoir_size 3 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric no --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   36636.51 sec.
    Max Memory :                                 343 MB
    Average Memory :                             258.41 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               15017.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                14
    Run time :                                   18753 sec.
    Turnaround time :                            18753 sec.

The output (if any) is above this job summary.

