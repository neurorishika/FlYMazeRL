
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GQLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_reward_set.csv
Model: GQNN_10-10-10_relu_acceptreject_asymmetric_qp_no-punishment_2022_10_04_02_25_55_496077
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6878	Validation Loss: 0.6762
Epoch 500: 	Training Loss: 0.6435	Validation Loss: 0.6093
Epoch 1000: 	Training Loss: 0.6644	Validation Loss: 0.6462
Epoch 1500: 	Training Loss: 0.6395	Validation Loss: 0.6188
Epoch 2000: 	Training Loss: 0.6442	Validation Loss: 0.6196
Epoch 2500: 	Training Loss: 0.6656	Validation Loss: 0.6452
Epoch 3000: 	Training Loss: 0.6666	Validation Loss: 0.6444
Early stopping at epoch 3076
Best validation loss: 0.5952
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6322	Validation Loss: 0.6342
Epoch 500: 	Training Loss: 0.6685	Validation Loss: 0.6322
Epoch 1000: 	Training Loss: 0.6374	Validation Loss: 0.6400
Epoch 1500: 	Training Loss: 0.6695	Validation Loss: 0.6300
Epoch 2000: 	Training Loss: 0.6520	Validation Loss: 0.6693
Epoch 2500: 	Training Loss: 0.6676	Validation Loss: 0.6306
Epoch 3000: 	Training Loss: 0.6277	Validation Loss: 0.6413
Early stopping at epoch 3387
Best validation loss: 0.6265
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6340	Validation Loss: 0.6242
Epoch 500: 	Training Loss: 0.6295	Validation Loss: 0.6309
Epoch 1000: 	Training Loss: 0.6306	Validation Loss: 0.6325
Epoch 1500: 	Training Loss: 0.6323	Validation Loss: 0.6382
Epoch 2000: 	Training Loss: 0.6348	Validation Loss: 0.6377
Epoch 2500: 	Training Loss: 0.7587	Validation Loss: 0.8452
Early stopping at epoch 2512
Best validation loss: 0.6226
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6316	Validation Loss: 0.6361
Epoch 500: 	Training Loss: 0.6569	Validation Loss: 0.6781
Epoch 1000: 	Training Loss: 0.6657	Validation Loss: 0.6816
Epoch 1500: 	Training Loss: 0.6535	Validation Loss: 0.6806
Epoch 2000: 	Training Loss: 0.6572	Validation Loss: 0.6741
Epoch 2500: 	Training Loss: 0.6354	Validation Loss: 0.6453
Early stopping at epoch 2522
Best validation loss: 0.6303
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6189	Validation Loss: 0.6823
Epoch 500: 	Training Loss: 0.6560	Validation Loss: 0.6828
Epoch 1000: 	Training Loss: 0.6544	Validation Loss: 0.6773
Epoch 1500: 	Training Loss: 0.6493	Validation Loss: 0.6855
Epoch 2000: 	Training Loss: 0.6469	Validation Loss: 0.6858
Epoch 2500: 	Training Loss: 0.6163	Validation Loss: 0.6796
Epoch 3000: 	Training Loss: 0.6250	Validation Loss: 0.6903
Epoch 3500: 	Training Loss: 0.6163	Validation Loss: 0.6744
Epoch 4000: 	Training Loss: 0.6685	Validation Loss: 0.6918
Epoch 4500: 	Training Loss: 0.6719	Validation Loss: 0.6899
Epoch 5000: 	Training Loss: 0.6505	Validation Loss: 0.6818
Epoch 5500: 	Training Loss: 0.6484	Validation Loss: 0.6776
Epoch 6000: 	Training Loss: 0.6420	Validation Loss: 0.6992
Epoch 6500: 	Training Loss: 0.6493	Validation Loss: 0.6734
Early stopping at epoch 6523
Best validation loss: 0.6680
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/GQNN_10-10-10_relu_acceptreject_asymmetric_qp_no-punishment_2022_10_04_02_25_55_496077/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u15>
Subject: Job 126236027: <GQNNx10x10x10-1-0> in cluster <Janelia> Done

Job <GQNNx10x10x10-1-0> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Tue Oct  4 02:25:30 2022
Job was executed on host(s) <e10u15>, in queue <local>, as user <mohantas> in cluster <Janelia> at Tue Oct  4 02:25:40 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Tue Oct  4 02:25:40 2022
Terminated at Tue Oct  4 12:38:04 2022
Results reported at Tue Oct  4 12:38:04 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_rajagopalan.py --agent GQNN --hidden_state_sizes 10 10 10 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric no --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   73158.62 sec.
    Max Memory :                                 263 MB
    Average Memory :                             243.75 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               15097.00 MB
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                14
    Run time :                                   36759 sec.
    Turnaround time :                            36754 sec.

The output (if any) is above this job summary.


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GQLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_reward_set.csv
Model: GQNN_10-10-10_relu_acceptreject_asymmetric_qp_no-punishment_2022_10_08_20_15_39_951822
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6727	Validation Loss: 0.6688
Epoch 500: 	Training Loss: 0.7503	Validation Loss: 0.7965
Epoch 1000: 	Training Loss: 0.6001	Validation Loss: 0.5946
Epoch 1500: 	Training Loss: 0.6032	Validation Loss: 0.5960
Epoch 2000: 	Training Loss: 0.6018	Validation Loss: 0.6012
Epoch 2500: 	Training Loss: 0.5995	Validation Loss: 0.5971
Epoch 3000: 	Training Loss: 0.6004	Validation Loss: 0.5946
Epoch 3500: 	Training Loss: 0.5984	Validation Loss: 0.5959
Epoch 4000: 	Training Loss: 0.6008	Validation Loss: 0.5950
Epoch 4500: 	Training Loss: 0.6007	Validation Loss: 0.5941
Epoch 5000: 	Training Loss: 0.5989	Validation Loss: 0.5930
Epoch 5500: 	Training Loss: 0.6008	Validation Loss: 0.5965
Epoch 6000: 	Training Loss: 0.6004	Validation Loss: 0.6009
Epoch 6500: 	Training Loss: 0.5987	Validation Loss: 0.5972
Epoch 7000: 	Training Loss: 0.5993	Validation Loss: 0.5962
Early stopping at epoch 7255
Best validation loss: 0.5916
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6053	Validation Loss: 0.5801
Epoch 500: 	Training Loss: 0.6067	Validation Loss: 0.5820
Epoch 1000: 	Training Loss: 0.6048	Validation Loss: 0.5801
Epoch 1500: 	Training Loss: 0.6610	Validation Loss: 0.6573
Epoch 2000: 	Training Loss: 0.6083	Validation Loss: 0.5818
Epoch 2500: 	Training Loss: 0.6046	Validation Loss: 0.5777
Early stopping at epoch 2534
Best validation loss: 0.5771
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6081	Validation Loss: 0.5625
Epoch 500: 	Training Loss: 0.6079	Validation Loss: 0.5644
Epoch 1000: 	Training Loss: 0.6081	Validation Loss: 0.5650
Epoch 1500: 	Training Loss: 0.6099	Validation Loss: 0.5644
Epoch 2000: 	Training Loss: 0.6081	Validation Loss: 0.5634
Epoch 2500: 	Training Loss: 0.6075	Validation Loss: 0.5830
Early stopping at epoch 2515
Best validation loss: 0.5615
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.5926	Validation Loss: 0.6250
Epoch 500: 	Training Loss: 0.7595	Validation Loss: 0.7607
Epoch 1000: 	Training Loss: 0.5919	Validation Loss: 0.6304
Epoch 1500: 	Training Loss: 0.5927	Validation Loss: 0.6248
Epoch 2000: 	Training Loss: 0.5939	Validation Loss: 0.6223
Epoch 2500: 	Training Loss: 0.5971	Validation Loss: 0.6252
Epoch 3000: 	Training Loss: 0.5930	Validation Loss: 0.6272
Epoch 3500: 	Training Loss: 0.5937	Validation Loss: 0.6235
Epoch 4000: 	Training Loss: 0.5945	Validation Loss: 0.6241
Early stopping at epoch 4021
Best validation loss: 0.6200
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.5972	Validation Loss: 0.6008
Epoch 500: 	Training Loss: 0.6007	Validation Loss: 0.6042
Epoch 1000: 	Training Loss: 0.6019	Validation Loss: 0.6038
Epoch 1500: 	Training Loss: 0.5978	Validation Loss: 0.6017
Epoch 2000: 	Training Loss: 0.5967	Validation Loss: 0.6057
Epoch 2500: 	Training Loss: 0.5968	Validation Loss: 0.6037
Early stopping at epoch 2500
Best validation loss: 0.6008
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/GQNN_10-10-10_relu_acceptreject_asymmetric_qp_no-punishment_2022_10_08_20_15_39_951822/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u14>
Subject: Job 126381407: <GQNNx10x10x10-1-0> in cluster <Janelia> Done

Job <GQNNx10x10x10-1-0> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Sat Oct  8 20:15:30 2022
Job was executed on host(s) <e10u14>, in queue <local>, as user <mohantas> in cluster <Janelia> at Sat Oct  8 20:15:31 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Sat Oct  8 20:15:31 2022
Terminated at Sun Oct  9 23:20:15 2022
Results reported at Sun Oct  9 23:20:15 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_mohanta.py --agent GQNN --hidden_state_sizes 10 10 10 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric no --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   193640.97 sec.
    Max Memory :                                 358 MB
    Average Memory :                             261.73 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               15002.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                14
    Run time :                                   97488 sec.
    Turnaround time :                            97485 sec.

The output (if any) is above this job summary.

