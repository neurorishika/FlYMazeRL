
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GQLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_reward_set.csv
Model: GQNN_2-2_relu_acceptreject_symmetric_qp_no-punishment_2022_10_04_02_25_14_618293
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6936	Validation Loss: 0.6932
Epoch 500: 	Training Loss: 0.6314	Validation Loss: 0.6548
Epoch 1000: 	Training Loss: 0.6311	Validation Loss: 0.6555
Epoch 1500: 	Training Loss: 0.6235	Validation Loss: 0.6472
Epoch 2000: 	Training Loss: 0.6239	Validation Loss: 0.6476
Epoch 2500: 	Training Loss: 0.6232	Validation Loss: 0.6471
Epoch 3000: 	Training Loss: 0.6230	Validation Loss: 0.6488
Epoch 3500: 	Training Loss: 0.6230	Validation Loss: 0.6486
Epoch 4000: 	Training Loss: 0.6230	Validation Loss: 0.6490
Epoch 4500: 	Training Loss: 0.6243	Validation Loss: 0.6505
Epoch 5000: 	Training Loss: 0.6232	Validation Loss: 0.6491
Epoch 5500: 	Training Loss: 0.6226	Validation Loss: 0.6527
Early stopping at epoch 5669
Best validation loss: 0.6463
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6331	Validation Loss: 0.6301
Epoch 500: 	Training Loss: 0.6292	Validation Loss: 0.6289
Epoch 1000: 	Training Loss: 0.6289	Validation Loss: 0.6290
Epoch 1500: 	Training Loss: 0.6286	Validation Loss: 0.6287
Epoch 2000: 	Training Loss: 0.6295	Validation Loss: 0.6302
Epoch 2500: 	Training Loss: 0.6277	Validation Loss: 0.6314
Early stopping at epoch 2505
Best validation loss: 0.6278
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6405	Validation Loss: 0.5977
Epoch 500: 	Training Loss: 0.6365	Validation Loss: 0.6023
Epoch 1000: 	Training Loss: 0.6493	Validation Loss: 0.6048
Epoch 1500: 	Training Loss: 0.6372	Validation Loss: 0.6024
Epoch 2000: 	Training Loss: 0.6364	Validation Loss: 0.6013
Epoch 2500: 	Training Loss: 0.6399	Validation Loss: 0.6004
Early stopping at epoch 2506
Best validation loss: 0.5965
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6379	Validation Loss: 0.6022
Epoch 500: 	Training Loss: 0.6363	Validation Loss: 0.6034
Epoch 1000: 	Training Loss: 0.6369	Validation Loss: 0.6045
Epoch 1500: 	Training Loss: 0.6359	Validation Loss: 0.6092
Epoch 2000: 	Training Loss: 0.6556	Validation Loss: 0.6482
Epoch 2500: 	Training Loss: 0.6418	Validation Loss: 0.6124
Early stopping at epoch 2550
Best validation loss: 0.6001
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6273	Validation Loss: 0.6528
Epoch 500: 	Training Loss: 0.6245	Validation Loss: 0.6531
Epoch 1000: 	Training Loss: 0.6225	Validation Loss: 0.6534
Epoch 1500: 	Training Loss: 0.6217	Validation Loss: 0.6528
Epoch 2000: 	Training Loss: 0.6218	Validation Loss: 0.6525
Epoch 2500: 	Training Loss: 0.6216	Validation Loss: 0.6533
Epoch 3000: 	Training Loss: 0.6213	Validation Loss: 0.6533
Early stopping at epoch 3410
Best validation loss: 0.6521
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/GQNN_2-2_relu_acceptreject_symmetric_qp_no-punishment_2022_10_04_02_25_14_618293/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u14>
Subject: Job 126235706: <GQNNx2x2-1-1> in cluster <Janelia> Done

Job <GQNNx2x2-1-1> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Tue Oct  4 02:24:54 2022
Job was executed on host(s) <e10u14>, in queue <local>, as user <mohantas> in cluster <Janelia> at Tue Oct  4 02:24:56 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Tue Oct  4 02:24:56 2022
Terminated at Tue Oct  4 09:46:02 2022
Results reported at Tue Oct  4 09:46:02 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_rajagopalan.py --agent GQNN --hidden_state_sizes 2 2 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric yes --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   52726.45 sec.
    Max Memory :                                 261 MB
    Average Memory :                             244.42 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               15099.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                14
    Run time :                                   26468 sec.
    Turnaround time :                            26468 sec.

The output (if any) is above this job summary.


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GQLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_reward_set.csv
Model: GQNN_2-2_relu_acceptreject_symmetric_qp_no-punishment_2022_10_08_20_15_11_526686
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6724	Validation Loss: 0.6537
Epoch 500: 	Training Loss: 0.6050	Validation Loss: 0.5805
Epoch 1000: 	Training Loss: 0.6051	Validation Loss: 0.5791
Epoch 1500: 	Training Loss: 0.6053	Validation Loss: 0.5792
Epoch 2000: 	Training Loss: 0.6050	Validation Loss: 0.5795
Epoch 2500: 	Training Loss: 0.6045	Validation Loss: 0.5804
Epoch 3000: 	Training Loss: 0.6051	Validation Loss: 0.5810
Epoch 3500: 	Training Loss: 0.6052	Validation Loss: 0.5788
Epoch 4000: 	Training Loss: 0.6047	Validation Loss: 0.5810
Epoch 4500: 	Training Loss: 0.6055	Validation Loss: 0.5781
Epoch 5000: 	Training Loss: 0.6043	Validation Loss: 0.5782
Epoch 5500: 	Training Loss: 0.6069	Validation Loss: 0.5824
Epoch 6000: 	Training Loss: 0.6046	Validation Loss: 0.5840
Epoch 6500: 	Training Loss: 0.6047	Validation Loss: 0.5791
Epoch 7000: 	Training Loss: 0.6043	Validation Loss: 0.5779
Epoch 7500: 	Training Loss: 0.6048	Validation Loss: 0.5787
Epoch 8000: 	Training Loss: 0.6069	Validation Loss: 0.5798
Epoch 8500: 	Training Loss: 0.6065	Validation Loss: 0.5793
Epoch 9000: 	Training Loss: 0.6055	Validation Loss: 0.5795
Epoch 9500: 	Training Loss: 0.6055	Validation Loss: 0.5794
Epoch 10000: 	Training Loss: 0.6058	Validation Loss: 0.5818
Epoch 10500: 	Training Loss: 0.6056	Validation Loss: 0.5793
Epoch 11000: 	Training Loss: 0.6047	Validation Loss: 0.5787
Epoch 11500: 	Training Loss: 0.6042	Validation Loss: 0.5780
Early stopping at epoch 11713
Best validation loss: 0.5769
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.5997	Validation Loss: 0.6033
Epoch 500: 	Training Loss: 0.5993	Validation Loss: 0.6012
Epoch 1000: 	Training Loss: 0.6002	Validation Loss: 0.6056
Epoch 1500: 	Training Loss: 0.5992	Validation Loss: 0.5992
Epoch 2000: 	Training Loss: 0.5992	Validation Loss: 0.5993
Epoch 2500: 	Training Loss: 0.5991	Validation Loss: 0.6005
Epoch 3000: 	Training Loss: 0.5993	Validation Loss: 0.5992
Early stopping at epoch 3195
Best validation loss: 0.5984
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.5928	Validation Loss: 0.6242
Epoch 500: 	Training Loss: 0.5925	Validation Loss: 0.6255
Epoch 1000: 	Training Loss: 0.5922	Validation Loss: 0.6242
Epoch 1500: 	Training Loss: 0.5926	Validation Loss: 0.6248
Epoch 2000: 	Training Loss: 0.5920	Validation Loss: 0.6252
Epoch 2500: 	Training Loss: 0.5937	Validation Loss: 0.6305
Epoch 3000: 	Training Loss: 0.5928	Validation Loss: 0.6264
Early stopping at epoch 3274
Best validation loss: 0.6227
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.5966	Validation Loss: 0.6147
Epoch 500: 	Training Loss: 0.5953	Validation Loss: 0.6151
Epoch 1000: 	Training Loss: 0.5966	Validation Loss: 0.6144
Epoch 1500: 	Training Loss: 0.5961	Validation Loss: 0.6154
Epoch 2000: 	Training Loss: 0.5963	Validation Loss: 0.6157
Epoch 2500: 	Training Loss: 0.5952	Validation Loss: 0.6134
Early stopping at epoch 2514
Best validation loss: 0.6120
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.5963	Validation Loss: 0.6096
Epoch 500: 	Training Loss: 0.5979	Validation Loss: 0.6114
Epoch 1000: 	Training Loss: 0.5969	Validation Loss: 0.6097
Epoch 1500: 	Training Loss: 0.5963	Validation Loss: 0.6102
Epoch 2000: 	Training Loss: 0.5964	Validation Loss: 0.6098
Epoch 2500: 	Training Loss: 0.5997	Validation Loss: 0.6093
Epoch 3000: 	Training Loss: 0.5957	Validation Loss: 0.6127
Epoch 3500: 	Training Loss: 0.5974	Validation Loss: 0.6087
Epoch 4000: 	Training Loss: 0.5965	Validation Loss: 0.6138
Epoch 4500: 	Training Loss: 0.5973	Validation Loss: 0.6119
Early stopping at epoch 4506
Best validation loss: 0.6074
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/GQNN_2-2_relu_acceptreject_symmetric_qp_no-punishment_2022_10_08_20_15_11_526686/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u03>
Subject: Job 126381355: <GQNNx2x2-1-1> in cluster <Janelia> Done

Job <GQNNx2x2-1-1> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Sat Oct  8 20:14:55 2022
Job was executed on host(s) <e10u03>, in queue <local>, as user <mohantas> in cluster <Janelia> at Sat Oct  8 20:14:55 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Sat Oct  8 20:14:55 2022
Terminated at Tue Oct 11 03:36:58 2022
Results reported at Tue Oct 11 03:36:58 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_mohanta.py --agent GQNN --hidden_state_sizes 2 2 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric yes --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   395593.78 sec.
    Max Memory :                                 414 MB
    Average Memory :                             271.17 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               14946.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                14
    Run time :                                   199325 sec.
    Turnaround time :                            199323 sec.

The output (if any) is above this job summary.

