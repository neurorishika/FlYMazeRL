
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GRNNLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_reward_set.csv
Model: GRNN_1x2_RNN_acceptreject_asymmetric_qp_no-punishment_2022_10_04_02_17_35_280754
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.7015	Validation Loss: 0.6988
Epoch 500: 	Training Loss: 0.6283	Validation Loss: 0.6122
Epoch 1000: 	Training Loss: 0.6280	Validation Loss: 0.6124
Epoch 1500: 	Training Loss: 0.6303	Validation Loss: 0.6139
Epoch 2000: 	Training Loss: 0.6306	Validation Loss: 0.6110
Epoch 2500: 	Training Loss: 0.6283	Validation Loss: 0.6106
Epoch 3000: 	Training Loss: 0.6288	Validation Loss: 0.6111
Epoch 3500: 	Training Loss: 0.6281	Validation Loss: 0.6119
Epoch 4000: 	Training Loss: 0.6277	Validation Loss: 0.6118
Epoch 4500: 	Training Loss: 0.6277	Validation Loss: 0.6124
Early stopping at epoch 4520
Best validation loss: 0.6095
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6963	Validation Loss: 0.6952
Epoch 500: 	Training Loss: 0.6357	Validation Loss: 0.5994
Epoch 1000: 	Training Loss: 0.6333	Validation Loss: 0.5975
Epoch 1500: 	Training Loss: 0.6331	Validation Loss: 0.5954
Epoch 2000: 	Training Loss: 0.6350	Validation Loss: 0.5979
Epoch 2500: 	Training Loss: 0.6328	Validation Loss: 0.6001
Epoch 3000: 	Training Loss: 0.6327	Validation Loss: 0.5975
Early stopping at epoch 3078
Best validation loss: 0.5931
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6963	Validation Loss: 0.6944
Epoch 500: 	Training Loss: 0.6265	Validation Loss: 0.6559
Epoch 1000: 	Training Loss: 0.6255	Validation Loss: 0.6552
Epoch 1500: 	Training Loss: 0.6253	Validation Loss: 0.6550
Epoch 2000: 	Training Loss: 0.6249	Validation Loss: 0.6554
Epoch 2500: 	Training Loss: 0.6249	Validation Loss: 0.6531
Epoch 3000: 	Training Loss: 0.6217	Validation Loss: 0.6435
Epoch 3500: 	Training Loss: 0.6206	Validation Loss: 0.6439
Epoch 4000: 	Training Loss: 0.6311	Validation Loss: 0.6505
Epoch 4500: 	Training Loss: 0.6206	Validation Loss: 0.6434
Epoch 5000: 	Training Loss: 0.6194	Validation Loss: 0.6526
Epoch 5500: 	Training Loss: 0.6199	Validation Loss: 0.6454
Epoch 6000: 	Training Loss: 0.6200	Validation Loss: 0.6465
Epoch 6500: 	Training Loss: 0.6200	Validation Loss: 0.6440
Epoch 7000: 	Training Loss: 0.6211	Validation Loss: 0.6446
Epoch 7500: 	Training Loss: 0.6211	Validation Loss: 0.6435
Early stopping at epoch 7935
Best validation loss: 0.6424
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6894	Validation Loss: 0.6881
Epoch 500: 	Training Loss: 0.6183	Validation Loss: 0.6448
Epoch 1000: 	Training Loss: 0.6171	Validation Loss: 0.6455
Epoch 1500: 	Training Loss: 0.6179	Validation Loss: 0.6500
Epoch 2000: 	Training Loss: 0.6192	Validation Loss: 0.6515
Epoch 2500: 	Training Loss: 0.6190	Validation Loss: 0.6519
Epoch 3000: 	Training Loss: 0.6169	Validation Loss: 0.6456
Epoch 3500: 	Training Loss: 0.6166	Validation Loss: 0.6452
Epoch 4000: 	Training Loss: 0.6178	Validation Loss: 0.6451
Epoch 4500: 	Training Loss: 0.6173	Validation Loss: 0.6450
Epoch 5000: 	Training Loss: 0.6166	Validation Loss: 0.6450
Epoch 5500: 	Training Loss: 0.6174	Validation Loss: 0.6446
Early stopping at epoch 5771
Best validation loss: 0.6437
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6927	Validation Loss: 0.6920
Epoch 500: 	Training Loss: 0.6351	Validation Loss: 0.6165
Epoch 1000: 	Training Loss: 0.6353	Validation Loss: 0.6096
Epoch 1500: 	Training Loss: 0.6359	Validation Loss: 0.6032
Epoch 2000: 	Training Loss: 0.6363	Validation Loss: 0.6006
Epoch 2500: 	Training Loss: 0.6342	Validation Loss: 0.6009
Epoch 3000: 	Training Loss: 0.6363	Validation Loss: 0.5973
Epoch 3500: 	Training Loss: 0.6354	Validation Loss: 0.6005
Epoch 4000: 	Training Loss: 0.6341	Validation Loss: 0.6032
Epoch 4500: 	Training Loss: 0.6349	Validation Loss: 0.5995
Epoch 5000: 	Training Loss: 0.6355	Validation Loss: 0.6005
Epoch 5500: 	Training Loss: 0.6351	Validation Loss: 0.5999
Epoch 6000: 	Training Loss: 0.6377	Validation Loss: 0.5973
Epoch 6500: 	Training Loss: 0.6344	Validation Loss: 0.6001
Epoch 7000: 	Training Loss: 0.6448	Validation Loss: 0.6026
Epoch 7500: 	Training Loss: 0.6356	Validation Loss: 0.6036
Early stopping at epoch 7953
Best validation loss: 0.5967
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/GRNN_1x2_RNN_acceptreject_asymmetric_qp_no-punishment_2022_10_04_02_17_35_280754/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u13>
Subject: Job 126232881: <GRNNx2-2-0> in cluster <Janelia> Done

Job <GRNNx2-2-0> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Tue Oct  4 02:17:03 2022
Job was executed on host(s) <e10u13>, in queue <local>, as user <mohantas> in cluster <Janelia> at Tue Oct  4 02:17:03 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Tue Oct  4 02:17:03 2022
Terminated at Tue Oct  4 04:40:16 2022
Results reported at Tue Oct  4 04:40:16 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_rajagopalan.py --agent GRNN --num_reservoir 1 --reservoir_size 2 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric no --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   291787.34 sec.
    Max Memory :                                 271 MB
    Average Memory :                             239.49 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               15089.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                60
    Run time :                                   8596 sec.
    Turnaround time :                            8593 sec.

The output (if any) is above this job summary.


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GRNNLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_reward_set.csv
Model: GRNN_1x2_RNN_acceptreject_asymmetric_qp_no-punishment_2022_10_08_20_14_37_243672
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6902	Validation Loss: 0.6876
Epoch 500: 	Training Loss: 0.6018	Validation Loss: 0.5928
Epoch 1000: 	Training Loss: 0.5994	Validation Loss: 0.6006
Epoch 1500: 	Training Loss: 0.6029	Validation Loss: 0.5966
Epoch 2000: 	Training Loss: 0.5996	Validation Loss: 0.5933
Epoch 2500: 	Training Loss: 0.6000	Validation Loss: 0.5917
Epoch 3000: 	Training Loss: 0.6000	Validation Loss: 0.5939
Epoch 3500: 	Training Loss: 0.6014	Validation Loss: 0.5953
Epoch 4000: 	Training Loss: 0.5994	Validation Loss: 0.5995
Epoch 4500: 	Training Loss: 0.6012	Validation Loss: 0.5955
Early stopping at epoch 4629
Best validation loss: 0.5898
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6994	Validation Loss: 0.6830
Epoch 500: 	Training Loss: 0.6059	Validation Loss: 0.5719
Epoch 1000: 	Training Loss: 0.6064	Validation Loss: 0.5685
Epoch 1500: 	Training Loss: 0.6056	Validation Loss: 0.5699
Epoch 2000: 	Training Loss: 0.6062	Validation Loss: 0.5721
Epoch 2500: 	Training Loss: 0.6063	Validation Loss: 0.5697
Epoch 3000: 	Training Loss: 0.6052	Validation Loss: 0.5686
Epoch 3500: 	Training Loss: 0.6070	Validation Loss: 0.5705
Epoch 4000: 	Training Loss: 0.6059	Validation Loss: 0.5731
Epoch 4500: 	Training Loss: 0.6052	Validation Loss: 0.5681
Epoch 5000: 	Training Loss: 0.6052	Validation Loss: 0.5683
Epoch 5500: 	Training Loss: 0.6071	Validation Loss: 0.5683
Early stopping at epoch 5670
Best validation loss: 0.5676
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6888	Validation Loss: 0.6760
Epoch 500: 	Training Loss: 0.5995	Validation Loss: 0.5975
Epoch 1000: 	Training Loss: 0.6003	Validation Loss: 0.5979
Epoch 1500: 	Training Loss: 0.6002	Validation Loss: 0.6018
Epoch 2000: 	Training Loss: 0.6011	Validation Loss: 0.5995
Epoch 2500: 	Training Loss: 0.6002	Validation Loss: 0.5976
Epoch 3000: 	Training Loss: 0.5994	Validation Loss: 0.5991
Epoch 3500: 	Training Loss: 0.6002	Validation Loss: 0.6031
Epoch 4000: 	Training Loss: 0.6010	Validation Loss: 0.5968
Early stopping at epoch 4219
Best validation loss: 0.5955
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6922	Validation Loss: 0.6955
Epoch 500: 	Training Loss: 0.5951	Validation Loss: 0.6177
Epoch 1000: 	Training Loss: 0.5961	Validation Loss: 0.6117
Epoch 1500: 	Training Loss: 0.5957	Validation Loss: 0.6128
Epoch 2000: 	Training Loss: 0.5973	Validation Loss: 0.6134
Epoch 2500: 	Training Loss: 0.5949	Validation Loss: 0.6124
Epoch 3000: 	Training Loss: 0.5943	Validation Loss: 0.6106
Epoch 3500: 	Training Loss: 0.5950	Validation Loss: 0.6118
Epoch 4000: 	Training Loss: 0.5960	Validation Loss: 0.6104
Early stopping at epoch 4134
Best validation loss: 0.6095
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6830	Validation Loss: 0.6637
Epoch 500: 	Training Loss: 0.6060	Validation Loss: 0.5782
Epoch 1000: 	Training Loss: 0.6056	Validation Loss: 0.5749
Epoch 1500: 	Training Loss: 0.6074	Validation Loss: 0.5794
Epoch 2000: 	Training Loss: 0.6059	Validation Loss: 0.5758
Epoch 2500: 	Training Loss: 0.6060	Validation Loss: 0.5848
Epoch 3000: 	Training Loss: 0.6059	Validation Loss: 0.5759
Early stopping at epoch 3351
Best validation loss: 0.5726
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/GRNN_1x2_RNN_acceptreject_asymmetric_qp_no-punishment_2022_10_08_20_14_37_243672/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u25>
Subject: Job 126381320: <GRNNx2-2-0> in cluster <Janelia> Done

Job <GRNNx2-2-0> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Sat Oct  8 20:14:30 2022
Job was executed on host(s) <e10u25>, in queue <local>, as user <mohantas> in cluster <Janelia> at Sat Oct  8 20:14:31 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Sat Oct  8 20:14:31 2022
Terminated at Sun Oct  9 00:25:59 2022
Results reported at Sun Oct  9 00:25:59 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_mohanta.py --agent GRNN --num_reservoir 1 --reservoir_size 2 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric no --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   29490.13 sec.
    Max Memory :                                 345 MB
    Average Memory :                             263.10 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               15015.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                14
    Run time :                                   15088 sec.
    Turnaround time :                            15089 sec.

The output (if any) is above this job summary.

