
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GQLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_reward_set.csv
Model: GQNN_2-2_relu_acceptreject_symmetric_qp_no-punishment_2022_10_04_02_25_14_618294
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6851	Validation Loss: 0.6803
Epoch 500: 	Training Loss: 0.6313	Validation Loss: 0.6398
Epoch 1000: 	Training Loss: 0.6311	Validation Loss: 0.6400
Epoch 1500: 	Training Loss: 0.6275	Validation Loss: 0.6392
Epoch 2000: 	Training Loss: 0.6282	Validation Loss: 0.6386
Epoch 2500: 	Training Loss: 0.6276	Validation Loss: 0.6373
Epoch 3000: 	Training Loss: 0.6274	Validation Loss: 0.6383
Epoch 3500: 	Training Loss: 0.6281	Validation Loss: 0.6395
Epoch 4000: 	Training Loss: 0.6270	Validation Loss: 0.6381
Epoch 4500: 	Training Loss: 0.6269	Validation Loss: 0.6389
Epoch 5000: 	Training Loss: 0.6291	Validation Loss: 0.6373
Epoch 5500: 	Training Loss: 0.6291	Validation Loss: 0.6359
Epoch 6000: 	Training Loss: 0.6283	Validation Loss: 0.6373
Epoch 6500: 	Training Loss: 0.6282	Validation Loss: 0.6354
Epoch 7000: 	Training Loss: 0.6271	Validation Loss: 0.6380
Epoch 7500: 	Training Loss: 0.6257	Validation Loss: 0.6383
Epoch 8000: 	Training Loss: 0.6307	Validation Loss: 0.6393
Epoch 8500: 	Training Loss: 0.6255	Validation Loss: 0.6402
Epoch 9000: 	Training Loss: 0.6259	Validation Loss: 0.6375
Epoch 9500: 	Training Loss: 0.6251	Validation Loss: 0.6401
Epoch 10000: 	Training Loss: 0.6262	Validation Loss: 0.6369
Epoch 10500: 	Training Loss: 0.6265	Validation Loss: 0.6364
Epoch 11000: 	Training Loss: 0.6253	Validation Loss: 0.6383
Epoch 11500: 	Training Loss: 0.6251	Validation Loss: 0.6398
Epoch 12000: 	Training Loss: 0.6254	Validation Loss: 0.6367
Epoch 12500: 	Training Loss: 0.6254	Validation Loss: 0.6369
Early stopping at epoch 12769
Best validation loss: 0.6330
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6312	Validation Loss: 0.6274
Epoch 500: 	Training Loss: 0.6279	Validation Loss: 0.6326
Epoch 1000: 	Training Loss: 0.6273	Validation Loss: 0.6306
Epoch 1500: 	Training Loss: 0.6277	Validation Loss: 0.6320
Epoch 2000: 	Training Loss: 0.6269	Validation Loss: 0.6316
Epoch 2500: 	Training Loss: 0.6269	Validation Loss: 0.6308
Early stopping at epoch 2501
Best validation loss: 0.6265
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6266	Validation Loss: 0.6406
Epoch 500: 	Training Loss: 0.6240	Validation Loss: 0.6483
Epoch 1000: 	Training Loss: 0.6234	Validation Loss: 0.6472
Epoch 1500: 	Training Loss: 0.6233	Validation Loss: 0.6448
Epoch 2000: 	Training Loss: 0.6245	Validation Loss: 0.6454
Epoch 2500: 	Training Loss: 0.6236	Validation Loss: 0.6472
Early stopping at epoch 2502
Best validation loss: 0.6403
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6264	Validation Loss: 0.6419
Epoch 500: 	Training Loss: 0.6246	Validation Loss: 0.6415
Epoch 1000: 	Training Loss: 0.6242	Validation Loss: 0.6439
Epoch 1500: 	Training Loss: 0.6236	Validation Loss: 0.6442
Epoch 2000: 	Training Loss: 0.6238	Validation Loss: 0.6443
Epoch 2500: 	Training Loss: 0.6232	Validation Loss: 0.6453
Early stopping at epoch 2522
Best validation loss: 0.6384
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6228	Validation Loss: 0.6509
Epoch 500: 	Training Loss: 0.6223	Validation Loss: 0.6500
Epoch 1000: 	Training Loss: 0.6214	Validation Loss: 0.6547
Epoch 1500: 	Training Loss: 0.6218	Validation Loss: 0.6538
Epoch 2000: 	Training Loss: 0.6215	Validation Loss: 0.6541
Epoch 2500: 	Training Loss: 0.6212	Validation Loss: 0.6542
Early stopping at epoch 2524
Best validation loss: 0.6491
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/GQNN_2-2_relu_acceptreject_symmetric_qp_no-punishment_2022_10_04_02_25_14_618294/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u14>
Subject: Job 126235700: <GQNNx2x2-0-1> in cluster <Janelia> Done

Job <GQNNx2x2-0-1> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Tue Oct  4 02:24:54 2022
Job was executed on host(s) <e10u14>, in queue <local>, as user <mohantas> in cluster <Janelia> at Tue Oct  4 02:24:56 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Tue Oct  4 02:24:56 2022
Terminated at Tue Oct  4 11:49:02 2022
Results reported at Tue Oct  4 11:49:02 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_rajagopalan.py --agent GQNN --hidden_state_sizes 2 2 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric yes --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   67437.34 sec.
    Max Memory :                                 283 MB
    Average Memory :                             252.67 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               15077.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                14
    Run time :                                   33848 sec.
    Turnaround time :                            33848 sec.

The output (if any) is above this job summary.


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GQLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_reward_set.csv
Model: GQNN_2-2_relu_acceptreject_symmetric_qp_no-punishment_2022_10_08_20_15_11_574467
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6858	Validation Loss: 0.6795
Epoch 500: 	Training Loss: 0.6112	Validation Loss: 0.5827
Epoch 1000: 	Training Loss: 0.6055	Validation Loss: 0.5764
Epoch 1500: 	Training Loss: 0.6056	Validation Loss: 0.5756
Epoch 2000: 	Training Loss: 0.6121	Validation Loss: 0.5828
Epoch 2500: 	Training Loss: 0.6059	Validation Loss: 0.5758
Epoch 3000: 	Training Loss: 0.6070	Validation Loss: 0.5760
Epoch 3500: 	Training Loss: 0.6215	Validation Loss: 0.5815
Epoch 4000: 	Training Loss: 0.6068	Validation Loss: 0.5777
Epoch 4500: 	Training Loss: 0.6081	Validation Loss: 0.5756
Epoch 5000: 	Training Loss: 0.6062	Validation Loss: 0.5765
Epoch 5500: 	Training Loss: 0.6082	Validation Loss: 0.5770
Epoch 6000: 	Training Loss: 0.6061	Validation Loss: 0.5771
Early stopping at epoch 6446
Best validation loss: 0.5748
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.5964	Validation Loss: 0.6182
Epoch 500: 	Training Loss: 0.5973	Validation Loss: 0.6141
Epoch 1000: 	Training Loss: 0.5958	Validation Loss: 0.6171
Epoch 1500: 	Training Loss: 0.6020	Validation Loss: 0.6253
Epoch 2000: 	Training Loss: 0.5963	Validation Loss: 0.6135
Epoch 2500: 	Training Loss: 0.5980	Validation Loss: 0.6146
Early stopping at epoch 2561
Best validation loss: 0.6130
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6016	Validation Loss: 0.5949
Epoch 500: 	Training Loss: 0.6014	Validation Loss: 0.5982
Epoch 1000: 	Training Loss: 0.6032	Validation Loss: 0.5952
Epoch 1500: 	Training Loss: 0.6010	Validation Loss: 0.5970
Epoch 2000: 	Training Loss: 0.6022	Validation Loss: 0.5957
Epoch 2500: 	Training Loss: 0.6014	Validation Loss: 0.5954
Early stopping at epoch 2826
Best validation loss: 0.5937
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.5939	Validation Loss: 0.6249
Epoch 500: 	Training Loss: 0.5933	Validation Loss: 0.6254
Epoch 1000: 	Training Loss: 0.5948	Validation Loss: 0.6267
Epoch 1500: 	Training Loss: 0.5996	Validation Loss: 0.6319
Epoch 2000: 	Training Loss: 0.5982	Validation Loss: 0.6308
Epoch 2500: 	Training Loss: 0.5926	Validation Loss: 0.6246
Epoch 3000: 	Training Loss: 0.5929	Validation Loss: 0.6262
Early stopping at epoch 3095
Best validation loss: 0.6240
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6005	Validation Loss: 0.5938
Epoch 500: 	Training Loss: 0.6005	Validation Loss: 0.5937
Epoch 1000: 	Training Loss: 0.6027	Validation Loss: 0.5940
Epoch 1500: 	Training Loss: 0.6060	Validation Loss: 0.5944
Epoch 2000: 	Training Loss: 0.6004	Validation Loss: 0.5945
Epoch 2500: 	Training Loss: 0.6011	Validation Loss: 0.5943
Epoch 3000: 	Training Loss: 0.6009	Validation Loss: 0.5939
Epoch 3500: 	Training Loss: 0.6034	Validation Loss: 0.5987
Epoch 4000: 	Training Loss: 0.6009	Validation Loss: 0.5938
Early stopping at epoch 4092
Best validation loss: 0.5923
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/GQNN_2-2_relu_acceptreject_symmetric_qp_no-punishment_2022_10_08_20_15_11_574467/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u03>
Subject: Job 126381354: <GQNNx2x2-0-1> in cluster <Janelia> Done

Job <GQNNx2x2-0-1> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Sat Oct  8 20:14:55 2022
Job was executed on host(s) <e10u03>, in queue <local>, as user <mohantas> in cluster <Janelia> at Sat Oct  8 20:14:55 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Sat Oct  8 20:14:55 2022
Terminated at Mon Oct 10 18:00:02 2022
Results reported at Mon Oct 10 18:00:02 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_mohanta.py --agent GQNN --hidden_state_sizes 2 2 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric yes --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   326737.28 sec.
    Max Memory :                                 348 MB
    Average Memory :                             261.14 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               15012.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                14
    Run time :                                   164709 sec.
    Turnaround time :                            164707 sec.

The output (if any) is above this job summary.

