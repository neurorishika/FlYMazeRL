
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GRNNLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_reward_set.csv
Model: GRNN_1x2_RNN_acceptreject_symmetric_qp_no-punishment_2022_10_04_02_23_24_775963
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6863	Validation Loss: 0.6822
Epoch 500: 	Training Loss: 0.6301	Validation Loss: 0.6057
Epoch 1000: 	Training Loss: 0.6295	Validation Loss: 0.6026
Epoch 1500: 	Training Loss: 0.6296	Validation Loss: 0.6044
Epoch 2000: 	Training Loss: 0.6427	Validation Loss: 0.6206
Epoch 2500: 	Training Loss: 0.6294	Validation Loss: 0.6046
Epoch 3000: 	Training Loss: 0.6379	Validation Loss: 0.6140
Epoch 3500: 	Training Loss: 0.6331	Validation Loss: 0.6101
Early stopping at epoch 3518
Best validation loss: 0.6024
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6983	Validation Loss: 0.6966
Epoch 500: 	Training Loss: 0.6253	Validation Loss: 0.6246
Epoch 1000: 	Training Loss: 0.6257	Validation Loss: 0.6274
Epoch 1500: 	Training Loss: 0.6262	Validation Loss: 0.6191
Epoch 2000: 	Training Loss: 0.6257	Validation Loss: 0.6231
Epoch 2500: 	Training Loss: 0.6247	Validation Loss: 0.6243
Epoch 3000: 	Training Loss: 0.6248	Validation Loss: 0.6240
Epoch 3500: 	Training Loss: 0.6247	Validation Loss: 0.6296
Epoch 4000: 	Training Loss: 0.6243	Validation Loss: 0.6309
Epoch 4500: 	Training Loss: 0.6239	Validation Loss: 0.6291
Epoch 5000: 	Training Loss: 0.6238	Validation Loss: 0.6222
Epoch 5500: 	Training Loss: 0.6349	Validation Loss: 0.6261
Epoch 6000: 	Training Loss: 0.6375	Validation Loss: 0.6289
Epoch 6500: 	Training Loss: 0.6255	Validation Loss: 0.6333
Epoch 7000: 	Training Loss: 0.6254	Validation Loss: 0.6343
Epoch 7500: 	Training Loss: 0.6253	Validation Loss: 0.6359
Early stopping at epoch 7704
Best validation loss: 0.6127
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6859	Validation Loss: 0.6736
Epoch 500: 	Training Loss: 0.6298	Validation Loss: 0.6158
Epoch 1000: 	Training Loss: 0.6318	Validation Loss: 0.6131
Epoch 1500: 	Training Loss: 0.6309	Validation Loss: 0.6152
Epoch 2000: 	Training Loss: 0.6308	Validation Loss: 0.6133
Epoch 2500: 	Training Loss: 0.6314	Validation Loss: 0.6146
Epoch 3000: 	Training Loss: 0.6313	Validation Loss: 0.6157
Epoch 3500: 	Training Loss: 0.6320	Validation Loss: 0.6146
Early stopping at epoch 3736
Best validation loss: 0.6096
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6914	Validation Loss: 0.6898
Epoch 500: 	Training Loss: 0.6270	Validation Loss: 0.6201
Epoch 1000: 	Training Loss: 0.6242	Validation Loss: 0.6209
Epoch 1500: 	Training Loss: 0.6242	Validation Loss: 0.6222
Epoch 2000: 	Training Loss: 0.6269	Validation Loss: 0.6191
Epoch 2500: 	Training Loss: 0.6334	Validation Loss: 0.6193
Epoch 3000: 	Training Loss: 0.6356	Validation Loss: 0.6215
Epoch 3500: 	Training Loss: 0.6303	Validation Loss: 0.6211
Epoch 4000: 	Training Loss: 0.6303	Validation Loss: 0.6219
Epoch 4500: 	Training Loss: 0.6303	Validation Loss: 0.6204
Early stopping at epoch 4587
Best validation loss: 0.6166
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6840	Validation Loss: 0.6787
Epoch 500: 	Training Loss: 0.6372	Validation Loss: 0.6226
Epoch 1000: 	Training Loss: 0.6264	Validation Loss: 0.6138
Epoch 1500: 	Training Loss: 0.6282	Validation Loss: 0.6147
Epoch 2000: 	Training Loss: 0.6259	Validation Loss: 0.6135
Epoch 2500: 	Training Loss: 0.6285	Validation Loss: 0.6175
Epoch 3000: 	Training Loss: 0.6306	Validation Loss: 0.6225
Epoch 3500: 	Training Loss: 0.6270	Validation Loss: 0.6184
Epoch 4000: 	Training Loss: 0.6261	Validation Loss: 0.6139
Epoch 4500: 	Training Loss: 0.6262	Validation Loss: 0.6179
Early stopping at epoch 4560
Best validation loss: 0.6104
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/GRNN_1x2_RNN_acceptreject_symmetric_qp_no-punishment_2022_10_04_02_23_24_775963/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u30>
Subject: Job 126235122: <GRNNx2-1-1> in cluster <Janelia> Done

Job <GRNNx2-1-1> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Tue Oct  4 02:23:19 2022
Job was executed on host(s) <e10u30>, in queue <local>, as user <mohantas> in cluster <Janelia> at Tue Oct  4 02:23:19 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Tue Oct  4 02:23:19 2022
Terminated at Tue Oct  4 05:13:17 2022
Results reported at Tue Oct  4 05:13:17 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_rajagopalan.py --agent GRNN --num_reservoir 1 --reservoir_size 2 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric yes --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   20284.16 sec.
    Max Memory :                                 260 MB
    Average Memory :                             237.60 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               15100.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                14
    Run time :                                   10198 sec.
    Turnaround time :                            10198 sec.

The output (if any) is above this job summary.


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GRNNLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_reward_set.csv
Model: GRNN_1x2_RNN_acceptreject_symmetric_qp_no-punishment_2022_10_08_20_12_09_106855
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6893	Validation Loss: 0.6812
Epoch 500: 	Training Loss: 0.5975	Validation Loss: 0.6064
Epoch 1000: 	Training Loss: 0.5971	Validation Loss: 0.6061
Epoch 1500: 	Training Loss: 0.5974	Validation Loss: 0.6107
Epoch 2000: 	Training Loss: 0.5969	Validation Loss: 0.6052
Epoch 2500: 	Training Loss: 0.5994	Validation Loss: 0.6115
Epoch 3000: 	Training Loss: 0.5973	Validation Loss: 0.6057
Epoch 3500: 	Training Loss: 0.5974	Validation Loss: 0.6083
Epoch 4000: 	Training Loss: 0.5971	Validation Loss: 0.6053
Early stopping at epoch 4383
Best validation loss: 0.6043
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6776	Validation Loss: 0.6678
Epoch 500: 	Training Loss: 0.5930	Validation Loss: 0.6300
Epoch 1000: 	Training Loss: 0.5935	Validation Loss: 0.6243
Epoch 1500: 	Training Loss: 0.5935	Validation Loss: 0.6259
Epoch 2000: 	Training Loss: 0.5933	Validation Loss: 0.6272
Epoch 2500: 	Training Loss: 0.5942	Validation Loss: 0.6258
Early stopping at epoch 2840
Best validation loss: 0.6231
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6870	Validation Loss: 0.6847
Epoch 500: 	Training Loss: 0.5924	Validation Loss: 0.6283
Epoch 1000: 	Training Loss: 0.5924	Validation Loss: 0.6294
Epoch 1500: 	Training Loss: 0.5925	Validation Loss: 0.6281
Epoch 2000: 	Training Loss: 0.5926	Validation Loss: 0.6289
Epoch 2500: 	Training Loss: 0.5922	Validation Loss: 0.6276
Epoch 3000: 	Training Loss: 0.5923	Validation Loss: 0.6280
Early stopping at epoch 3020
Best validation loss: 0.6271
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6762	Validation Loss: 0.6637
Epoch 500: 	Training Loss: 0.5944	Validation Loss: 0.6229
Epoch 1000: 	Training Loss: 0.5941	Validation Loss: 0.6203
Epoch 1500: 	Training Loss: 0.5950	Validation Loss: 0.6244
Epoch 2000: 	Training Loss: 0.5936	Validation Loss: 0.6225
Epoch 2500: 	Training Loss: 0.5942	Validation Loss: 0.6209
Epoch 3000: 	Training Loss: 0.5947	Validation Loss: 0.6212
Epoch 3500: 	Training Loss: 0.5933	Validation Loss: 0.6209
Epoch 4000: 	Training Loss: 0.5942	Validation Loss: 0.6226
Early stopping at epoch 4296
Best validation loss: 0.6195
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.7028	Validation Loss: 0.6948
Epoch 500: 	Training Loss: 0.5903	Validation Loss: 0.6395
Epoch 1000: 	Training Loss: 0.5891	Validation Loss: 0.6384
Epoch 1500: 	Training Loss: 0.5885	Validation Loss: 0.6408
Epoch 2000: 	Training Loss: 0.5901	Validation Loss: 0.6386
Epoch 2500: 	Training Loss: 0.5886	Validation Loss: 0.6404
Epoch 3000: 	Training Loss: 0.5887	Validation Loss: 0.6405
Early stopping at epoch 3371
Best validation loss: 0.6378
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/GRNN_1x2_RNN_acceptreject_symmetric_qp_no-punishment_2022_10_08_20_12_09_106855/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u09>
Subject: Job 126381295: <GRNNx2-1-1> in cluster <Janelia> Done

Job <GRNNx2-1-1> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Sat Oct  8 20:11:29 2022
Job was executed on host(s) <e10u09>, in queue <local>, as user <mohantas> in cluster <Janelia> at Sat Oct  8 20:11:29 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Sat Oct  8 20:11:29 2022
Terminated at Sun Oct  9 07:36:54 2022
Results reported at Sun Oct  9 07:36:54 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_mohanta.py --agent GRNN --num_reservoir 1 --reservoir_size 2 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric yes --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1003050.62 sec.
    Max Memory :                                 332 MB
    Average Memory :                             254.90 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               15028.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                60
    Run time :                                   41125 sec.
    Turnaround time :                            41125 sec.

The output (if any) is above this job summary.

