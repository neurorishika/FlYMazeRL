/groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash/../nn_fitting_rajagopalan.py:336: RuntimeWarning: divide by zero encountered in log
  np.log(1 - pred_action_prob[obs_actions == 0])
/groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash/../nn_fitting_rajagopalan.py:336: RuntimeWarning: divide by zero encountered in log
  np.log(1 - pred_action_prob[obs_actions == 0])
/groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash/../nn_fitting_rajagopalan.py:336: RuntimeWarning: divide by zero encountered in log
  np.log(1 - pred_action_prob[obs_actions == 0])
/groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash/../nn_fitting_rajagopalan.py:336: RuntimeWarning: divide by zero encountered in log
  np.log(1 - pred_action_prob[obs_actions == 0])
/groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash/../nn_fitting_rajagopalan.py:336: RuntimeWarning: divide by zero encountered in log
  np.log(1 - pred_action_prob[obs_actions == 0])
/groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash/../nn_fitting_rajagopalan.py:336: RuntimeWarning: divide by zero encountered in log
  np.log(1 - pred_action_prob[obs_actions == 0])
/groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash/../nn_fitting_rajagopalan.py:336: RuntimeWarning: divide by zero encountered in log
  np.log(1 - pred_action_prob[obs_actions == 0])
/groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash/../nn_fitting_rajagopalan.py:336: RuntimeWarning: divide by zero encountered in log
  np.log(1 - pred_action_prob[obs_actions == 0])
/groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash/../nn_fitting_rajagopalan.py:336: RuntimeWarning: divide by zero encountered in log
  np.log(1 - pred_action_prob[obs_actions == 0])
/groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash/../nn_fitting_rajagopalan.py:336: RuntimeWarning: divide by zero encountered in log
  np.log(1 - pred_action_prob[obs_actions == 0])

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GRNNLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_reward_set.csv
Model: GRNN_1x100_RNN_acceptreject_asymmetric_qp_no-punishment_2022_10_04_02_17_35_326988
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6792	Validation Loss: 0.6702
Epoch 500: 	Training Loss: 0.6257	Validation Loss: 0.6284
Epoch 1000: 	Training Loss: 0.6292	Validation Loss: 0.6277
Epoch 1500: 	Training Loss: 0.6385	Validation Loss: 0.6483
Epoch 2000: 	Training Loss: 0.6275	Validation Loss: 0.6382
Epoch 2500: 	Training Loss: 0.6411	Validation Loss: 0.6471
Epoch 3000: 	Training Loss: 0.6436	Validation Loss: 0.6530
Early stopping at epoch 3101
Best validation loss: 0.6198
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6818	Validation Loss: 0.6680
Epoch 500: 	Training Loss: 0.6100	Validation Loss: 0.6678
Epoch 1000: 	Training Loss: 0.6474	Validation Loss: 0.6704
Epoch 1500: 	Training Loss: 0.6251	Validation Loss: 0.6805
Epoch 2000: 	Training Loss: 0.6230	Validation Loss: 0.6606
Epoch 2500: 	Training Loss: 0.6412	Validation Loss: 0.6702
Epoch 3000: 	Training Loss: 0.6328	Validation Loss: 0.6624
Epoch 3500: 	Training Loss: 0.6348	Validation Loss: 0.6641
Epoch 4000: 	Training Loss: 0.6899	Validation Loss: 0.6916
Epoch 4500: 	Training Loss: 0.6113	Validation Loss: 0.6725
Epoch 5000: 	Training Loss: 0.6393	Validation Loss: 0.6691
Early stopping at epoch 5164
Best validation loss: 0.6458
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6812	Validation Loss: 0.6636
Epoch 500: 	Training Loss: 0.6407	Validation Loss: 0.6607
Epoch 1000: 	Training Loss: 0.6433	Validation Loss: 0.6652
Epoch 1500: 	Training Loss: 0.6332	Validation Loss: 0.6543
Epoch 2000: 	Training Loss: 0.6179	Validation Loss: 0.6674
Epoch 2500: 	Training Loss: 0.6398	Validation Loss: 0.6468
Early stopping at epoch 2516
Best validation loss: 0.6296
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6776	Validation Loss: 0.6474
Epoch 500: 	Training Loss: 0.6541	Validation Loss: 0.6599
Epoch 1000: 	Training Loss: 0.6290	Validation Loss: 0.6406
Epoch 1500: 	Training Loss: 0.6376	Validation Loss: 0.6500
Epoch 2000: 	Training Loss: 0.6036	Validation Loss: 0.6419
Epoch 2500: 	Training Loss: 0.6371	Validation Loss: 0.6420
Epoch 3000: 	Training Loss: 0.6505	Validation Loss: 0.6518
Epoch 3500: 	Training Loss: 0.6311	Validation Loss: 0.6522
Early stopping at epoch 3794
Best validation loss: 0.6242
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6854	Validation Loss: 0.6733
Epoch 500: 	Training Loss: 0.6241	Validation Loss: 0.6632
Epoch 1000: 	Training Loss: 0.6200	Validation Loss: 0.6644
Epoch 1500: 	Training Loss: 0.6599	Validation Loss: 0.6694
Epoch 2000: 	Training Loss: 0.6382	Validation Loss: 0.6774
Epoch 2500: 	Training Loss: 0.6280	Validation Loss: 0.6791
Epoch 3000: 	Training Loss: 0.6039	Validation Loss: 0.7105
Epoch 3500: 	Training Loss: 0.6369	Validation Loss: 0.6672
Epoch 4000: 	Training Loss: 0.6517	Validation Loss: 0.6926
Epoch 4500: 	Training Loss: 0.6514	Validation Loss: 0.6686
Early stopping at epoch 4778
Best validation loss: 0.6515
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/GRNN_1x100_RNN_acceptreject_asymmetric_qp_no-punishment_2022_10_04_02_17_35_326988/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u30>
Subject: Job 126232942: <GRNNx100-3-0> in cluster <Janelia> Done

Job <GRNNx100-3-0> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Tue Oct  4 02:17:12 2022
Job was executed on host(s) <e10u30>, in queue <local>, as user <mohantas> in cluster <Janelia> at Tue Oct  4 02:17:12 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Tue Oct  4 02:17:12 2022
Terminated at Tue Oct  4 04:22:06 2022
Results reported at Tue Oct  4 04:22:06 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_rajagopalan.py --agent GRNN --num_reservoir 1 --reservoir_size 100 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric no --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   14860.63 sec.
    Max Memory :                                 254 MB
    Average Memory :                             236.97 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               15106.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                14
    Run time :                                   7497 sec.
    Turnaround time :                            7494 sec.

The output (if any) is above this job summary.


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GRNNLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_reward_set.csv
Model: GRNN_1x100_RNN_acceptreject_asymmetric_qp_no-punishment_2022_10_08_20_14_45_134385
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6598	Validation Loss: 0.6279
Epoch 500: 	Training Loss: 0.6248	Validation Loss: 0.6276
Epoch 1000: 	Training Loss: 0.6931	Validation Loss: 0.6931
Epoch 1500: 	Training Loss: 0.6646	Validation Loss: 0.6525
Epoch 2000: 	Training Loss: 0.6097	Validation Loss: 0.6117
Epoch 2500: 	Training Loss: 0.6138	Validation Loss: 0.6247
Epoch 3000: 	Training Loss: 0.6190	Validation Loss: 0.6268
Epoch 3500: 	Training Loss: 0.6212	Validation Loss: 0.6187
Epoch 4000: 	Training Loss: 0.6287	Validation Loss: 0.6336
Epoch 4500: 	Training Loss: 0.6931	Validation Loss: 0.6931
Early stopping at epoch 4536
Best validation loss: 0.6028
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6938	Validation Loss: 0.6892
Epoch 500: 	Training Loss: 0.6275	Validation Loss: 0.6218
Epoch 1000: 	Training Loss: 0.6947	Validation Loss: 0.6935
Epoch 1500: 	Training Loss: 0.6312	Validation Loss: 0.5843
Epoch 2000: 	Training Loss: 0.6930	Validation Loss: 0.6931
Epoch 2500: 	Training Loss: 0.6255	Validation Loss: 0.5686
Epoch 3000: 	Training Loss: 0.6926	Validation Loss: 0.6925
Epoch 3500: 	Training Loss: 0.6250	Validation Loss: 0.5857
Epoch 4000: 	Training Loss: 0.6931	Validation Loss: 0.6930
Epoch 4500: 	Training Loss: 0.7717	Validation Loss: 0.8002
Early stopping at epoch 4961
Best validation loss: 0.5624
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6424	Validation Loss: 0.5969
Epoch 500: 	Training Loss: 0.6146	Validation Loss: 0.6078
Epoch 1000: 	Training Loss: 0.6677	Validation Loss: 0.6904
Epoch 1500: 	Training Loss: 0.6167	Validation Loss: 0.5915
Epoch 2000: 	Training Loss: 0.6244	Validation Loss: 0.5978
Epoch 2500: 	Training Loss: 0.6184	Validation Loss: 0.5929
Epoch 3000: 	Training Loss: 0.6192	Validation Loss: 0.5884
Epoch 3500: 	Training Loss: 0.6119	Validation Loss: 0.5867
Epoch 4000: 	Training Loss: 0.7585	Validation Loss: 0.7612
Epoch 4500: 	Training Loss: 0.7573	Validation Loss: 0.7614
Epoch 5000: 	Training Loss: 0.6931	Validation Loss: 0.6932
Epoch 5500: 	Training Loss: 0.6164	Validation Loss: 0.5877
Epoch 6000: 	Training Loss: 0.6706	Validation Loss: 0.6533
Early stopping at epoch 6036
Best validation loss: 0.5785
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6469	Validation Loss: 0.6152
Epoch 500: 	Training Loss: 0.6369	Validation Loss: 0.6810
Epoch 1000: 	Training Loss: 0.6930	Validation Loss: 0.6930
Epoch 1500: 	Training Loss: 0.6098	Validation Loss: 0.6076
Epoch 2000: 	Training Loss: 0.6032	Validation Loss: 0.6078
Epoch 2500: 	Training Loss: 0.6933	Validation Loss: 0.6931
Epoch 3000: 	Training Loss: 0.7231	Validation Loss: 0.7334
Epoch 3500: 	Training Loss: 0.7237	Validation Loss: 0.7315
Epoch 4000: 	Training Loss: 0.6929	Validation Loss: 0.6925
Epoch 4500: 	Training Loss: 0.6142	Validation Loss: 0.6187
Early stopping at epoch 4560
Best validation loss: 0.6013
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6623	Validation Loss: 0.6380
Epoch 500: 	Training Loss: 0.6122	Validation Loss: 0.5798
Epoch 1000: 	Training Loss: 0.7679	Validation Loss: 0.7636
Epoch 1500: 	Training Loss: 0.6119	Validation Loss: 0.5842
Epoch 2000: 	Training Loss: 0.6153	Validation Loss: 0.5956
Epoch 2500: 	Training Loss: 0.6191	Validation Loss: 0.5885
Epoch 3000: 	Training Loss: 0.6286	Validation Loss: 0.6079
Epoch 3500: 	Training Loss: 0.6610	Validation Loss: 0.6289
Early stopping at epoch 3950
Best validation loss: 0.5788
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/GRNN_1x100_RNN_acceptreject_asymmetric_qp_no-punishment_2022_10_08_20_14_45_134385/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u10>
Subject: Job 126381337: <GRNNx100-3-0> in cluster <Janelia> Done

Job <GRNNx100-3-0> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Sat Oct  8 20:14:39 2022
Job was executed on host(s) <e10u10>, in queue <local>, as user <mohantas> in cluster <Janelia> at Sat Oct  8 20:14:39 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Sat Oct  8 20:14:39 2022
Terminated at Sun Oct  9 02:28:36 2022
Results reported at Sun Oct  9 02:28:36 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_mohanta.py --agent GRNN --num_reservoir 1 --reservoir_size 100 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric no --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   44018.73 sec.
    Max Memory :                                 371 MB
    Average Memory :                             286.05 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               14989.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                14
    Run time :                                   22435 sec.
    Turnaround time :                            22437 sec.

The output (if any) is above this job summary.

