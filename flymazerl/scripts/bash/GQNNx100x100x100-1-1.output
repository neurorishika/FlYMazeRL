
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GQLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_reward_set.csv
Model: GQNN_100-100-100_relu_acceptreject_symmetric_qp_no-punishment_2022_10_04_02_25_11_505111
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6592	Validation Loss: 0.6545
Epoch 500: 	Training Loss: 0.6320	Validation Loss: 0.6357
Epoch 1000: 	Training Loss: 0.6308	Validation Loss: 0.6310
Epoch 1500: 	Training Loss: 0.6320	Validation Loss: 0.6335
Epoch 2000: 	Training Loss: 0.6298	Validation Loss: 0.6330
Epoch 2500: 	Training Loss: 0.6931	Validation Loss: 0.6932
Epoch 3000: 	Training Loss: 0.6931	Validation Loss: 0.6931
Epoch 3500: 	Training Loss: 0.6278	Validation Loss: 0.6337
Epoch 4000: 	Training Loss: 0.6273	Validation Loss: 0.6311
Epoch 4500: 	Training Loss: 0.6284	Validation Loss: 0.6303
Epoch 5000: 	Training Loss: 0.6931	Validation Loss: 0.6931
Epoch 5500: 	Training Loss: 0.6931	Validation Loss: 0.6932
Epoch 6000: 	Training Loss: 0.6263	Validation Loss: 0.6273
Epoch 6500: 	Training Loss: 0.6275	Validation Loss: 0.6289
Epoch 7000: 	Training Loss: 0.6261	Validation Loss: 0.6293
Epoch 7500: 	Training Loss: 0.6407	Validation Loss: 0.6451
Epoch 8000: 	Training Loss: 0.6318	Validation Loss: 0.6321
Epoch 8500: 	Training Loss: 0.6295	Validation Loss: 0.6305
Early stopping at epoch 8516
Best validation loss: 0.6254
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6245	Validation Loss: 0.6531
Epoch 500: 	Training Loss: 0.6931	Validation Loss: 0.6931
Epoch 1000: 	Training Loss: 0.6184	Validation Loss: 0.6574
Epoch 1500: 	Training Loss: 0.6204	Validation Loss: 0.6567
Epoch 2000: 	Training Loss: 0.6723	Validation Loss: 0.6727
Epoch 2500: 	Training Loss: 0.6931	Validation Loss: 0.6931
Early stopping at epoch 2506
Best validation loss: 0.6526
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6207	Validation Loss: 0.6598
Epoch 500: 	Training Loss: 0.6200	Validation Loss: 0.6615
Epoch 1000: 	Training Loss: 0.6197	Validation Loss: 0.6642
Epoch 1500: 	Training Loss: 0.6232	Validation Loss: 0.6596
Epoch 2000: 	Training Loss: 0.6230	Validation Loss: 0.6629
Epoch 2500: 	Training Loss: 0.6205	Validation Loss: 0.6679
Epoch 3000: 	Training Loss: 0.6215	Validation Loss: 0.6620
Epoch 3500: 	Training Loss: 0.6930	Validation Loss: 0.6928
Epoch 4000: 	Training Loss: 0.6930	Validation Loss: 0.6928
Early stopping at epoch 4314
Best validation loss: 0.6563
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6181	Validation Loss: 0.6699
Epoch 500: 	Training Loss: 0.6213	Validation Loss: 0.6797
Epoch 1000: 	Training Loss: 0.6209	Validation Loss: 0.6774
Epoch 1500: 	Training Loss: 0.6141	Validation Loss: 0.6745
Epoch 2000: 	Training Loss: 0.6138	Validation Loss: 0.6773
Epoch 2500: 	Training Loss: 0.6170	Validation Loss: 0.6808
Epoch 3000: 	Training Loss: 0.6931	Validation Loss: 0.6931
Early stopping at epoch 3313
Best validation loss: 0.6647
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6547	Validation Loss: 0.6622
Epoch 500: 	Training Loss: 0.6262	Validation Loss: 0.6342
Epoch 1000: 	Training Loss: 0.6931	Validation Loss: 0.6931
Epoch 1500: 	Training Loss: 0.6932	Validation Loss: 0.6931
Epoch 2000: 	Training Loss: 0.6931	Validation Loss: 0.6931
Epoch 2500: 	Training Loss: 0.6300	Validation Loss: 0.6419
Epoch 3000: 	Training Loss: 0.6278	Validation Loss: 0.6407
Epoch 3500: 	Training Loss: 0.6931	Validation Loss: 0.6931
Epoch 4000: 	Training Loss: 0.6284	Validation Loss: 0.6410
Epoch 4500: 	Training Loss: 0.6279	Validation Loss: 0.6359
Epoch 5000: 	Training Loss: 0.6273	Validation Loss: 0.6339
Early stopping at epoch 5484
Best validation loss: 0.6309
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/GQNN_100-100-100_relu_acceptreject_symmetric_qp_no-punishment_2022_10_04_02_25_11_505111/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u21>
Subject: Job 126235801: <GQNNx100x100x100-1-1> in cluster <Janelia> Done

Job <GQNNx100x100x100-1-1> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Tue Oct  4 02:25:05 2022
Job was executed on host(s) <e10u21>, in queue <local>, as user <mohantas> in cluster <Janelia> at Tue Oct  4 02:25:06 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Tue Oct  4 02:25:06 2022
Terminated at Tue Oct  4 16:37:57 2022
Results reported at Tue Oct  4 16:37:57 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_rajagopalan.py --agent GQNN --hidden_state_sizes 100 100 100 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric yes --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   101635.23 sec.
    Max Memory :                                 282 MB
    Average Memory :                             256.00 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               15078.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                14
    Run time :                                   51174 sec.
    Turnaround time :                            51172 sec.

The output (if any) is above this job summary.


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GQLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_reward_set.csv
Model: GQNN_100-100-100_relu_acceptreject_symmetric_qp_no-punishment_2022_10_08_20_15_15_070553
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6405	Validation Loss: 0.5963
Epoch 500: 	Training Loss: 0.6034	Validation Loss: 0.6016
Epoch 1000: 	Training Loss: 0.6108	Validation Loss: 0.5978
Epoch 1500: 	Training Loss: 0.6931	Validation Loss: 0.6931
Epoch 2000: 	Training Loss: 0.6028	Validation Loss: 0.5888
Epoch 2500: 	Training Loss: 0.6083	Validation Loss: 0.6112
Epoch 3000: 	Training Loss: 0.6167	Validation Loss: 0.5983
Epoch 3500: 	Training Loss: 0.6072	Validation Loss: 0.5903
Early stopping at epoch 3817
Best validation loss: 0.5864
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.5976	Validation Loss: 0.6253
Epoch 500: 	Training Loss: 0.5953	Validation Loss: 0.6204
Epoch 1000: 	Training Loss: 0.5942	Validation Loss: 0.6214
Epoch 1500: 	Training Loss: 0.5967	Validation Loss: 0.6222
Epoch 2000: 	Training Loss: 0.6303	Validation Loss: 0.6511
Epoch 2500: 	Training Loss: 0.6411	Validation Loss: 0.6555
Epoch 3000: 	Training Loss: 0.5991	Validation Loss: 0.6302
Epoch 3500: 	Training Loss: 0.6033	Validation Loss: 0.6263
Early stopping at epoch 3521
Best validation loss: 0.6184
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.5964	Validation Loss: 0.6162
Epoch 500: 	Training Loss: 0.5981	Validation Loss: 0.6146
Epoch 1000: 	Training Loss: 0.5979	Validation Loss: 0.6139
Epoch 1500: 	Training Loss: 0.6931	Validation Loss: 0.6931
Epoch 2000: 	Training Loss: 0.6931	Validation Loss: 0.6931
Epoch 2500: 	Training Loss: 0.6029	Validation Loss: 0.6329
Early stopping at epoch 2505
Best validation loss: 0.6114
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6040	Validation Loss: 0.5873
Epoch 500: 	Training Loss: 0.6207	Validation Loss: 0.6497
Epoch 1000: 	Training Loss: 0.6213	Validation Loss: 0.6035
Epoch 1500: 	Training Loss: 0.6034	Validation Loss: 0.5880
Epoch 2000: 	Training Loss: 0.6027	Validation Loss: 0.5915
Epoch 2500: 	Training Loss: 0.6483	Validation Loss: 0.6060
Epoch 3000: 	Training Loss: 0.6931	Validation Loss: 0.6931
Early stopping at epoch 3247
Best validation loss: 0.5862
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.5948	Validation Loss: 0.6199
Epoch 500: 	Training Loss: 0.5973	Validation Loss: 0.6374
Epoch 1000: 	Training Loss: 0.5944	Validation Loss: 0.6204
Epoch 1500: 	Training Loss: 0.6009	Validation Loss: 0.6306
Epoch 2000: 	Training Loss: 0.6931	Validation Loss: 0.6931
Epoch 2500: 	Training Loss: 0.6931	Validation Loss: 0.6931
Epoch 3000: 	Training Loss: 0.6931	Validation Loss: 0.6931
Early stopping at epoch 3478
Best validation loss: 0.6183
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/GQNN_100-100-100_relu_acceptreject_symmetric_qp_no-punishment_2022_10_08_20_15_15_070553/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u22>
Subject: Job 126381375: <GQNNx100x100x100-1-1> in cluster <Janelia> Done

Job <GQNNx100x100x100-1-1> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Sat Oct  8 20:15:06 2022
Job was executed on host(s) <e10u22>, in queue <local>, as user <mohantas> in cluster <Janelia> at Sat Oct  8 20:15:06 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Sat Oct  8 20:15:06 2022
Terminated at Tue Oct 11 09:07:09 2022
Results reported at Tue Oct 11 09:07:09 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_mohanta.py --agent GQNN --hidden_state_sizes 100 100 100 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric yes --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   429089.97 sec.
    Max Memory :                                 378 MB
    Average Memory :                             316.92 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               14982.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                14
    Run time :                                   219121 sec.
    Turnaround time :                            219123 sec.

The output (if any) is above this job summary.

