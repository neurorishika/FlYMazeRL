
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GQLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_reward_set.csv
Model: GQNN_2-2_relu_acceptreject_asymmetric_qp_no-punishment_2022_10_04_02_25_55_496082
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6968	Validation Loss: 0.6932
Epoch 500: 	Training Loss: 0.6359	Validation Loss: 0.6226
Epoch 1000: 	Training Loss: 0.6336	Validation Loss: 0.6228
Epoch 1500: 	Training Loss: 0.6422	Validation Loss: 0.6296
Epoch 2000: 	Training Loss: 0.6355	Validation Loss: 0.6232
Epoch 2500: 	Training Loss: 0.6339	Validation Loss: 0.6235
Epoch 3000: 	Training Loss: 0.6339	Validation Loss: 0.6238
Epoch 3500: 	Training Loss: 0.6356	Validation Loss: 0.6261
Epoch 4000: 	Training Loss: 0.6331	Validation Loss: 0.6226
Epoch 4500: 	Training Loss: 0.6330	Validation Loss: 0.6226
Epoch 5000: 	Training Loss: 0.6332	Validation Loss: 0.6222
Epoch 5500: 	Training Loss: 0.6303	Validation Loss: 0.6234
Epoch 6000: 	Training Loss: 0.6298	Validation Loss: 0.6264
Epoch 6500: 	Training Loss: 0.6405	Validation Loss: 0.6317
Epoch 7000: 	Training Loss: 0.6309	Validation Loss: 0.6240
Epoch 7500: 	Training Loss: 0.6301	Validation Loss: 0.6235
Epoch 8000: 	Training Loss: 0.6288	Validation Loss: 0.6250
Epoch 8500: 	Training Loss: 0.6297	Validation Loss: 0.6271
Epoch 9000: 	Training Loss: 0.6297	Validation Loss: 0.6251
Epoch 9500: 	Training Loss: 0.6290	Validation Loss: 0.6248
Early stopping at epoch 9900
Best validation loss: 0.6199
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6224	Validation Loss: 0.6594
Epoch 500: 	Training Loss: 0.6202	Validation Loss: 0.6616
Epoch 1000: 	Training Loss: 0.6209	Validation Loss: 0.6614
Epoch 1500: 	Training Loss: 0.6261	Validation Loss: 0.6736
Epoch 2000: 	Training Loss: 0.6186	Validation Loss: 0.6645
Epoch 2500: 	Training Loss: 0.6194	Validation Loss: 0.6633
Epoch 3000: 	Training Loss: 0.6205	Validation Loss: 0.6565
Early stopping at epoch 3003
Best validation loss: 0.6553
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6352	Validation Loss: 0.6127
Epoch 500: 	Training Loss: 0.6315	Validation Loss: 0.6123
Epoch 1000: 	Training Loss: 0.6320	Validation Loss: 0.6130
Epoch 1500: 	Training Loss: 0.6314	Validation Loss: 0.6152
Epoch 2000: 	Training Loss: 0.6331	Validation Loss: 0.6135
Epoch 2500: 	Training Loss: 0.6333	Validation Loss: 0.6142
Early stopping at epoch 2504
Best validation loss: 0.6112
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6338	Validation Loss: 0.6136
Epoch 500: 	Training Loss: 0.6323	Validation Loss: 0.6145
Epoch 1000: 	Training Loss: 0.6312	Validation Loss: 0.6146
Epoch 1500: 	Training Loss: 0.6324	Validation Loss: 0.6137
Epoch 2000: 	Training Loss: 0.6318	Validation Loss: 0.6144
Epoch 2500: 	Training Loss: 0.6314	Validation Loss: 0.6136
Early stopping at epoch 2502
Best validation loss: 0.6125
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6294	Validation Loss: 0.6382
Epoch 500: 	Training Loss: 0.6247	Validation Loss: 0.6383
Epoch 1000: 	Training Loss: 0.6297	Validation Loss: 0.6404
Epoch 1500: 	Training Loss: 0.6348	Validation Loss: 0.6436
Epoch 2000: 	Training Loss: 0.6237	Validation Loss: 0.6389
Epoch 2500: 	Training Loss: 0.6268	Validation Loss: 0.6399
Early stopping at epoch 2502
Best validation loss: 0.6341
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/GQNN_2-2_relu_acceptreject_asymmetric_qp_no-punishment_2022_10_04_02_25_55_496082/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u15>
Subject: Job 126235980: <GQNNx2x2-2-0> in cluster <Janelia> Done

Job <GQNNx2x2-2-0> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Tue Oct  4 02:25:22 2022
Job was executed on host(s) <e10u15>, in queue <local>, as user <mohantas> in cluster <Janelia> at Tue Oct  4 02:25:22 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Tue Oct  4 02:25:22 2022
Terminated at Tue Oct  4 12:13:27 2022
Results reported at Tue Oct  4 12:13:27 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_rajagopalan.py --agent GQNN --hidden_state_sizes 2 2 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric no --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   70212.72 sec.
    Max Memory :                                 268 MB
    Average Memory :                             242.04 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               15092.00 MB
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                14
    Run time :                                   35281 sec.
    Turnaround time :                            35285 sec.

The output (if any) is above this job summary.


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GQLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_reward_set.csv
Model: GQNN_2-2_relu_acceptreject_asymmetric_qp_no-punishment_2022_10_08_20_15_30_796345
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6551	Validation Loss: 0.6738
Epoch 500: 	Training Loss: 0.5911	Validation Loss: 0.6490
Epoch 1000: 	Training Loss: 0.5907	Validation Loss: 0.6387
Epoch 1500: 	Training Loss: 0.5920	Validation Loss: 0.6405
Epoch 2000: 	Training Loss: 0.5935	Validation Loss: 0.6399
Epoch 2500: 	Training Loss: 0.5901	Validation Loss: 0.6378
Early stopping at epoch 2694
Best validation loss: 0.6363
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6013	Validation Loss: 0.6017
Epoch 500: 	Training Loss: 0.5995	Validation Loss: 0.6032
Epoch 1000: 	Training Loss: 0.6005	Validation Loss: 0.6044
Epoch 1500: 	Training Loss: 0.6015	Validation Loss: 0.6007
Epoch 2000: 	Training Loss: 0.5986	Validation Loss: 0.6032
Epoch 2500: 	Training Loss: 0.5994	Validation Loss: 0.6014
Epoch 3000: 	Training Loss: 0.5999	Validation Loss: 0.6054
Epoch 3500: 	Training Loss: 0.6000	Validation Loss: 0.6040
Epoch 4000: 	Training Loss: 0.5998	Validation Loss: 0.5998
Early stopping at epoch 4411
Best validation loss: 0.5993
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.5997	Validation Loss: 0.6055
Epoch 500: 	Training Loss: 0.5991	Validation Loss: 0.6072
Epoch 1000: 	Training Loss: 0.5983	Validation Loss: 0.6055
Epoch 1500: 	Training Loss: 0.5977	Validation Loss: 0.6052
Epoch 2000: 	Training Loss: 0.5979	Validation Loss: 0.6070
Epoch 2500: 	Training Loss: 0.5965	Validation Loss: 0.6087
Epoch 3000: 	Training Loss: 0.5967	Validation Loss: 0.6053
Epoch 3500: 	Training Loss: 0.5969	Validation Loss: 0.6047
Epoch 4000: 	Training Loss: 0.5967	Validation Loss: 0.6059
Epoch 4500: 	Training Loss: 0.5998	Validation Loss: 0.6111
Epoch 5000: 	Training Loss: 0.5982	Validation Loss: 0.6087
Early stopping at epoch 5427
Best validation loss: 0.6040
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.5875	Validation Loss: 0.6475
Epoch 500: 	Training Loss: 0.6468	Validation Loss: 0.6668
Epoch 1000: 	Training Loss: 0.5867	Validation Loss: 0.6495
Epoch 1500: 	Training Loss: 0.5887	Validation Loss: 0.6476
Epoch 2000: 	Training Loss: 0.5878	Validation Loss: 0.6509
Epoch 2500: 	Training Loss: 0.5872	Validation Loss: 0.6511
Epoch 3000: 	Training Loss: 0.5858	Validation Loss: 0.6475
Early stopping at epoch 3247
Best validation loss: 0.6446
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.5971	Validation Loss: 0.6080
Epoch 500: 	Training Loss: 0.5964	Validation Loss: 0.6063
Epoch 1000: 	Training Loss: 0.5967	Validation Loss: 0.6145
Epoch 1500: 	Training Loss: 0.5981	Validation Loss: 0.6097
Epoch 2000: 	Training Loss: 0.5967	Validation Loss: 0.6099
Epoch 2500: 	Training Loss: 0.5983	Validation Loss: 0.6099
Epoch 3000: 	Training Loss: 0.5966	Validation Loss: 0.6066
Epoch 3500: 	Training Loss: 0.7719	Validation Loss: 0.7104
Epoch 4000: 	Training Loss: 0.7713	Validation Loss: 0.7097
Early stopping at epoch 4346
Best validation loss: 0.6061
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/GQNN_2-2_relu_acceptreject_asymmetric_qp_no-punishment_2022_10_08_20_15_30_796345/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u14>
Subject: Job 126381392: <GQNNx2x2-2-0> in cluster <Janelia> Done

Job <GQNNx2x2-2-0> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Sat Oct  8 20:15:22 2022
Job was executed on host(s) <e10u14>, in queue <local>, as user <mohantas> in cluster <Janelia> at Sat Oct  8 20:15:22 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Sat Oct  8 20:15:22 2022
Terminated at Sun Oct  9 20:38:28 2022
Results reported at Sun Oct  9 20:38:28 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_mohanta.py --agent GQNN --hidden_state_sizes 2 2 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric no --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   174233.36 sec.
    Max Memory :                                 344 MB
    Average Memory :                             260.51 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               15016.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                14
    Run time :                                   87788 sec.
    Turnaround time :                            87786 sec.

The output (if any) is above this job summary.

