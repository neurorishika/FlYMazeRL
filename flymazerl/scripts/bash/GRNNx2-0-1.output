
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GRNNLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_reward_set.csv
Model: GRNN_1x2_RNN_acceptreject_symmetric_qp_no-punishment_2022_10_04_02_23_22_857050
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.7128	Validation Loss: 0.7017
Epoch 500: 	Training Loss: 0.6119	Validation Loss: 0.6701
Epoch 1000: 	Training Loss: 0.6220	Validation Loss: 0.6749
Epoch 1500: 	Training Loss: 0.6144	Validation Loss: 0.6712
Epoch 2000: 	Training Loss: 0.6162	Validation Loss: 0.6778
Epoch 2500: 	Training Loss: 0.6150	Validation Loss: 0.6740
Early stopping at epoch 2556
Best validation loss: 0.6651
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6963	Validation Loss: 0.6933
Epoch 500: 	Training Loss: 0.6385	Validation Loss: 0.6100
Epoch 1000: 	Training Loss: 0.6330	Validation Loss: 0.5981
Epoch 1500: 	Training Loss: 0.6321	Validation Loss: 0.6018
Epoch 2000: 	Training Loss: 0.6364	Validation Loss: 0.6088
Epoch 2500: 	Training Loss: 0.6339	Validation Loss: 0.6169
Epoch 3000: 	Training Loss: 0.6430	Validation Loss: 0.6110
Early stopping at epoch 3423
Best validation loss: 0.5957
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6909	Validation Loss: 0.6889
Epoch 500: 	Training Loss: 0.6349	Validation Loss: 0.6184
Epoch 1000: 	Training Loss: 0.6323	Validation Loss: 0.6167
Epoch 1500: 	Training Loss: 0.6320	Validation Loss: 0.6202
Epoch 2000: 	Training Loss: 0.6331	Validation Loss: 0.6215
Epoch 2500: 	Training Loss: 0.6374	Validation Loss: 0.6187
Epoch 3000: 	Training Loss: 0.6315	Validation Loss: 0.6219
Epoch 3500: 	Training Loss: 0.6364	Validation Loss: 0.6192
Early stopping at epoch 3598
Best validation loss: 0.6145
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6998	Validation Loss: 0.6966
Epoch 500: 	Training Loss: 0.6353	Validation Loss: 0.6098
Epoch 1000: 	Training Loss: 0.6518	Validation Loss: 0.6170
Epoch 1500: 	Training Loss: 0.6376	Validation Loss: 0.6024
Epoch 2000: 	Training Loss: 0.6368	Validation Loss: 0.6151
Epoch 2500: 	Training Loss: 0.6356	Validation Loss: 0.6151
Epoch 3000: 	Training Loss: 0.6372	Validation Loss: 0.6066
Epoch 3500: 	Training Loss: 0.6351	Validation Loss: 0.6047
Epoch 4000: 	Training Loss: 0.6349	Validation Loss: 0.6055
Epoch 4500: 	Training Loss: 0.6305	Validation Loss: 0.5986
Epoch 5000: 	Training Loss: 0.6309	Validation Loss: 0.5992
Epoch 5500: 	Training Loss: 0.6304	Validation Loss: 0.5971
Epoch 6000: 	Training Loss: 0.6299	Validation Loss: 0.5973
Epoch 6500: 	Training Loss: 0.6304	Validation Loss: 0.5964
Epoch 7000: 	Training Loss: 0.6299	Validation Loss: 0.5963
Epoch 7500: 	Training Loss: 0.6295	Validation Loss: 0.5965
Epoch 8000: 	Training Loss: 0.6300	Validation Loss: 0.5973
Epoch 8500: 	Training Loss: 0.6297	Validation Loss: 0.5971
Early stopping at epoch 8712
Best validation loss: 0.5951
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6711	Validation Loss: 0.6684
Epoch 500: 	Training Loss: 0.6221	Validation Loss: 0.6337
Epoch 1000: 	Training Loss: 0.6221	Validation Loss: 0.6324
Epoch 1500: 	Training Loss: 0.6220	Validation Loss: 0.6340
Epoch 2000: 	Training Loss: 0.6209	Validation Loss: 0.6328
Epoch 2500: 	Training Loss: 0.6207	Validation Loss: 0.6330
Epoch 3000: 	Training Loss: 0.6207	Validation Loss: 0.6337
Epoch 3500: 	Training Loss: 0.6213	Validation Loss: 0.6349
Early stopping at epoch 3526
Best validation loss: 0.6303
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/GRNN_1x2_RNN_acceptreject_symmetric_qp_no-punishment_2022_10_04_02_23_22_857050/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u07>
Subject: Job 126235116: <GRNNx2-0-1> in cluster <Janelia> Done

Job <GRNNx2-0-1> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Tue Oct  4 02:23:19 2022
Job was executed on host(s) <e10u07>, in queue <local>, as user <mohantas> in cluster <Janelia> at Tue Oct  4 02:23:19 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Tue Oct  4 02:23:19 2022
Terminated at Tue Oct  4 05:03:55 2022
Results reported at Tue Oct  4 05:03:55 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_rajagopalan.py --agent GRNN --num_reservoir 1 --reservoir_size 2 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric yes --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   19162.29 sec.
    Max Memory :                                 256 MB
    Average Memory :                             231.96 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               15104.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                14
    Run time :                                   9639 sec.
    Turnaround time :                            9636 sec.

The output (if any) is above this job summary.


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GRNNLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_reward_set.csv
Model: GRNN_1x2_RNN_acceptreject_symmetric_qp_no-punishment_2022_10_08_20_12_09_106908
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6767	Validation Loss: 0.6538
Epoch 500: 	Training Loss: 0.6066	Validation Loss: 0.5732
Epoch 1000: 	Training Loss: 0.6060	Validation Loss: 0.5731
Epoch 1500: 	Training Loss: 0.6063	Validation Loss: 0.5732
Epoch 2000: 	Training Loss: 0.6064	Validation Loss: 0.5732
Epoch 2500: 	Training Loss: 0.6066	Validation Loss: 0.5735
Epoch 3000: 	Training Loss: 0.6060	Validation Loss: 0.5737
Epoch 3500: 	Training Loss: 0.6062	Validation Loss: 0.5736
Epoch 4000: 	Training Loss: 0.6061	Validation Loss: 0.5733
Early stopping at epoch 4380
Best validation loss: 0.5724
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6856	Validation Loss: 0.6672
Epoch 500: 	Training Loss: 0.6086	Validation Loss: 0.5638
Epoch 1000: 	Training Loss: 0.6094	Validation Loss: 0.5616
Epoch 1500: 	Training Loss: 0.6087	Validation Loss: 0.5621
Epoch 2000: 	Training Loss: 0.6088	Validation Loss: 0.5615
Epoch 2500: 	Training Loss: 0.6096	Validation Loss: 0.5618
Epoch 3000: 	Training Loss: 0.6093	Validation Loss: 0.5619
Early stopping at epoch 3189
Best validation loss: 0.5603
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6753	Validation Loss: 0.6683
Epoch 500: 	Training Loss: 0.5958	Validation Loss: 0.6177
Epoch 1000: 	Training Loss: 0.5941	Validation Loss: 0.6187
Epoch 1500: 	Training Loss: 0.5936	Validation Loss: 0.6192
Epoch 2000: 	Training Loss: 0.5947	Validation Loss: 0.6174
Epoch 2500: 	Training Loss: 0.5949	Validation Loss: 0.6193
Epoch 3000: 	Training Loss: 0.5947	Validation Loss: 0.6184
Early stopping at epoch 3024
Best validation loss: 0.6169
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6897	Validation Loss: 0.6799
Epoch 500: 	Training Loss: 0.5987	Validation Loss: 0.6046
Epoch 1000: 	Training Loss: 0.5996	Validation Loss: 0.6030
Epoch 1500: 	Training Loss: 0.6002	Validation Loss: 0.6016
Epoch 2000: 	Training Loss: 0.5984	Validation Loss: 0.6021
Epoch 2500: 	Training Loss: 0.5984	Validation Loss: 0.6018
Epoch 3000: 	Training Loss: 0.6016	Validation Loss: 0.6035
Epoch 3500: 	Training Loss: 0.5988	Validation Loss: 0.6021
Epoch 4000: 	Training Loss: 0.5989	Validation Loss: 0.6013
Epoch 4500: 	Training Loss: 0.5991	Validation Loss: 0.6020
Epoch 5000: 	Training Loss: 0.5990	Validation Loss: 0.6013
Epoch 5500: 	Training Loss: 0.5988	Validation Loss: 0.6023
Epoch 6000: 	Training Loss: 0.5987	Validation Loss: 0.6014
Epoch 6500: 	Training Loss: 0.5980	Validation Loss: 0.6030
Early stopping at epoch 6755
Best validation loss: 0.6000
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6566	Validation Loss: 0.6281
Epoch 500: 	Training Loss: 0.6054	Validation Loss: 0.5799
Epoch 1000: 	Training Loss: 0.6061	Validation Loss: 0.5810
Epoch 1500: 	Training Loss: 0.6045	Validation Loss: 0.5819
Epoch 2000: 	Training Loss: 0.6050	Validation Loss: 0.5842
Epoch 2500: 	Training Loss: 0.6050	Validation Loss: 0.5818
Epoch 3000: 	Training Loss: 0.6054	Validation Loss: 0.5814
Early stopping at epoch 3153
Best validation loss: 0.5776
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/GRNN_1x2_RNN_acceptreject_symmetric_qp_no-punishment_2022_10_08_20_12_09_106908/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u09>
Subject: Job 126381294: <GRNNx2-0-1> in cluster <Janelia> Done

Job <GRNNx2-0-1> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Sat Oct  8 20:11:28 2022
Job was executed on host(s) <e10u09>, in queue <local>, as user <mohantas> in cluster <Janelia> at Sat Oct  8 20:11:28 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Sat Oct  8 20:11:28 2022
Terminated at Sun Oct  9 08:20:51 2022
Results reported at Sun Oct  9 08:20:51 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_mohanta.py --agent GRNN --num_reservoir 1 --reservoir_size 2 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric yes --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1131569.62 sec.
    Max Memory :                                 360 MB
    Average Memory :                             258.76 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               15000.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                60
    Run time :                                   43762 sec.
    Turnaround time :                            43763 sec.

The output (if any) is above this job summary.

