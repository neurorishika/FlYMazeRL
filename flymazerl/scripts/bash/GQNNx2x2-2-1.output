
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GQLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_reward_set.csv
Model: GQNN_2-2_relu_acceptreject_symmetric_qp_no-punishment_2022_10_04_02_25_14_618291
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6900	Validation Loss: 0.6883
Epoch 500: 	Training Loss: 0.6382	Validation Loss: 0.6345
Epoch 1000: 	Training Loss: 0.6324	Validation Loss: 0.6208
Epoch 1500: 	Training Loss: 0.6320	Validation Loss: 0.6215
Epoch 2000: 	Training Loss: 0.6317	Validation Loss: 0.6240
Epoch 2500: 	Training Loss: 0.6314	Validation Loss: 0.6183
Epoch 3000: 	Training Loss: 0.6284	Validation Loss: 0.6209
Epoch 3500: 	Training Loss: 0.6303	Validation Loss: 0.6219
Epoch 4000: 	Training Loss: 0.6279	Validation Loss: 0.6213
Early stopping at epoch 4447
Best validation loss: 0.6145
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6251	Validation Loss: 0.6419
Epoch 500: 	Training Loss: 0.6256	Validation Loss: 0.6482
Epoch 1000: 	Training Loss: 0.6241	Validation Loss: 0.6439
Epoch 1500: 	Training Loss: 0.6242	Validation Loss: 0.6443
Epoch 2000: 	Training Loss: 0.6242	Validation Loss: 0.6443
Epoch 2500: 	Training Loss: 0.6247	Validation Loss: 0.6452
Epoch 3000: 	Training Loss: 0.6239	Validation Loss: 0.6434
Epoch 3500: 	Training Loss: 0.6239	Validation Loss: 0.6427
Epoch 4000: 	Training Loss: 0.6239	Validation Loss: 0.6430
Epoch 4500: 	Training Loss: 0.6245	Validation Loss: 0.6454
Epoch 5000: 	Training Loss: 0.6231	Validation Loss: 0.6450
Epoch 5500: 	Training Loss: 0.6215	Validation Loss: 0.6444
Epoch 6000: 	Training Loss: 0.6226	Validation Loss: 0.6462
Epoch 6500: 	Training Loss: 0.6216	Validation Loss: 0.6458
Early stopping at epoch 6734
Best validation loss: 0.6406
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6353	Validation Loss: 0.6253
Epoch 500: 	Training Loss: 0.6290	Validation Loss: 0.6254
Epoch 1000: 	Training Loss: 0.6290	Validation Loss: 0.6253
Epoch 1500: 	Training Loss: 0.6287	Validation Loss: 0.6250
Epoch 2000: 	Training Loss: 0.6291	Validation Loss: 0.6251
Epoch 2500: 	Training Loss: 0.6289	Validation Loss: 0.6251
Early stopping at epoch 2506
Best validation loss: 0.6235
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6264	Validation Loss: 0.6376
Epoch 500: 	Training Loss: 0.6252	Validation Loss: 0.6416
Epoch 1000: 	Training Loss: 0.6245	Validation Loss: 0.6388
Epoch 1500: 	Training Loss: 0.6251	Validation Loss: 0.6401
Epoch 2000: 	Training Loss: 0.6240	Validation Loss: 0.6406
Epoch 2500: 	Training Loss: 0.6228	Validation Loss: 0.6412
Early stopping at epoch 2503
Best validation loss: 0.6363
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6255	Validation Loss: 0.6400
Epoch 500: 	Training Loss: 0.6244	Validation Loss: 0.6427
Epoch 1000: 	Training Loss: 0.6240	Validation Loss: 0.6424
Epoch 1500: 	Training Loss: 0.6316	Validation Loss: 0.6458
Epoch 2000: 	Training Loss: 0.6251	Validation Loss: 0.6415
Epoch 2500: 	Training Loss: 0.6242	Validation Loss: 0.6405
Epoch 3000: 	Training Loss: 0.6235	Validation Loss: 0.6399
Epoch 3500: 	Training Loss: 0.6241	Validation Loss: 0.6394
Epoch 4000: 	Training Loss: 0.6231	Validation Loss: 0.6397
Epoch 4500: 	Training Loss: 0.6241	Validation Loss: 0.6421
Epoch 5000: 	Training Loss: 0.6232	Validation Loss: 0.6388
Epoch 5500: 	Training Loss: 0.6234	Validation Loss: 0.6385
Epoch 6000: 	Training Loss: 0.6236	Validation Loss: 0.6388
Epoch 6500: 	Training Loss: 0.6238	Validation Loss: 0.6389
Epoch 7000: 	Training Loss: 0.6243	Validation Loss: 0.6388
Epoch 7500: 	Training Loss: 0.6256	Validation Loss: 0.6392
Epoch 8000: 	Training Loss: 0.6253	Validation Loss: 0.6388
Epoch 8500: 	Training Loss: 0.6248	Validation Loss: 0.6383
Epoch 9000: 	Training Loss: 0.6251	Validation Loss: 0.6379
Epoch 9500: 	Training Loss: 0.6243	Validation Loss: 0.6381
Epoch 10000: 	Training Loss: 0.6253	Validation Loss: 0.6364
Epoch 10500: 	Training Loss: 0.6236	Validation Loss: 0.6385
Epoch 11000: 	Training Loss: 0.6237	Validation Loss: 0.6382
Epoch 11500: 	Training Loss: 0.6244	Validation Loss: 0.6390
Early stopping at epoch 11586
Best validation loss: 0.6359
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/GQNN_2-2_relu_acceptreject_symmetric_qp_no-punishment_2022_10_04_02_25_14_618291/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u14>
Subject: Job 126235712: <GQNNx2x2-2-1> in cluster <Janelia> Done

Job <GQNNx2x2-2-1> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Tue Oct  4 02:24:55 2022
Job was executed on host(s) <e10u14>, in queue <local>, as user <mohantas> in cluster <Janelia> at Tue Oct  4 02:24:56 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Tue Oct  4 02:24:56 2022
Terminated at Tue Oct  4 13:36:17 2022
Results reported at Tue Oct  4 13:36:17 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_rajagopalan.py --agent GQNN --hidden_state_sizes 2 2 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric yes --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   80248.88 sec.
    Max Memory :                                 282 MB
    Average Memory :                             251.75 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               15078.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                14
    Run time :                                   40283 sec.
    Turnaround time :                            40282 sec.

The output (if any) is above this job summary.


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GQLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_reward_set.csv
Model: GQNN_2-2_relu_acceptreject_symmetric_qp_no-punishment_2022_10_08_20_15_11_391138
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6775	Validation Loss: 0.6619
Epoch 500: 	Training Loss: 0.5994	Validation Loss: 0.6011
Epoch 1000: 	Training Loss: 0.6004	Validation Loss: 0.6024
Epoch 1500: 	Training Loss: 0.6003	Validation Loss: 0.6021
Epoch 2000: 	Training Loss: 0.5994	Validation Loss: 0.6019
Epoch 2500: 	Training Loss: 0.5995	Validation Loss: 0.6010
Early stopping at epoch 2915
Best validation loss: 0.6004
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.5948	Validation Loss: 0.6208
Epoch 500: 	Training Loss: 0.5944	Validation Loss: 0.6208
Epoch 1000: 	Training Loss: 0.5947	Validation Loss: 0.6207
Epoch 1500: 	Training Loss: 0.5945	Validation Loss: 0.6205
Epoch 2000: 	Training Loss: 0.5943	Validation Loss: 0.6210
Epoch 2500: 	Training Loss: 0.6931	Validation Loss: 0.6931
Epoch 3000: 	Training Loss: 0.5943	Validation Loss: 0.6201
Epoch 3500: 	Training Loss: 0.5943	Validation Loss: 0.6200
Epoch 4000: 	Training Loss: 0.5942	Validation Loss: 0.6195
Epoch 4500: 	Training Loss: 0.5946	Validation Loss: 0.6201
Early stopping at epoch 4688
Best validation loss: 0.6192
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.5982	Validation Loss: 0.6099
Epoch 500: 	Training Loss: 0.5983	Validation Loss: 0.6118
Epoch 1000: 	Training Loss: 0.5969	Validation Loss: 0.6092
Epoch 1500: 	Training Loss: 0.5968	Validation Loss: 0.6095
Epoch 2000: 	Training Loss: 0.5980	Validation Loss: 0.6103
Epoch 2500: 	Training Loss: 0.5975	Validation Loss: 0.6089
Early stopping at epoch 2809
Best validation loss: 0.6080
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.5951	Validation Loss: 0.6181
Epoch 500: 	Training Loss: 0.5949	Validation Loss: 0.6185
Epoch 1000: 	Training Loss: 0.5948	Validation Loss: 0.6186
Epoch 1500: 	Training Loss: 0.5962	Validation Loss: 0.6201
Epoch 2000: 	Training Loss: 0.5946	Validation Loss: 0.6186
Epoch 2500: 	Training Loss: 0.5948	Validation Loss: 0.6178
Epoch 3000: 	Training Loss: 0.5989	Validation Loss: 0.6314
Epoch 3500: 	Training Loss: 0.5984	Validation Loss: 0.6234
Epoch 4000: 	Training Loss: 0.5956	Validation Loss: 0.6192
Early stopping at epoch 4474
Best validation loss: 0.6169
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.5951	Validation Loss: 0.6204
Epoch 500: 	Training Loss: 0.5939	Validation Loss: 0.6220
Epoch 1000: 	Training Loss: 0.5939	Validation Loss: 0.6209
Epoch 1500: 	Training Loss: 0.5946	Validation Loss: 0.6213
Epoch 2000: 	Training Loss: 0.5961	Validation Loss: 0.6201
Epoch 2500: 	Training Loss: 0.5949	Validation Loss: 0.6206
Epoch 3000: 	Training Loss: 0.6093	Validation Loss: 0.6225
Early stopping at epoch 3340
Best validation loss: 0.6199
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/GQNN_2-2_relu_acceptreject_symmetric_qp_no-punishment_2022_10_08_20_15_11_391138/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u03>
Subject: Job 126381356: <GQNNx2x2-2-1> in cluster <Janelia> Done

Job <GQNNx2x2-2-1> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Sat Oct  8 20:14:56 2022
Job was executed on host(s) <e10u03>, in queue <local>, as user <mohantas> in cluster <Janelia> at Sat Oct  8 20:14:58 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Sat Oct  8 20:14:58 2022
Terminated at Mon Oct 10 17:13:12 2022
Results reported at Mon Oct 10 17:13:12 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_mohanta.py --agent GQNN --hidden_state_sizes 2 2 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric yes --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   320856.00 sec.
    Max Memory :                                 334 MB
    Average Memory :                             262.95 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               15026.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                14
    Run time :                                   161898 sec.
    Turnaround time :                            161896 sec.

The output (if any) is above this job summary.

