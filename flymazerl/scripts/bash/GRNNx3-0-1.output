
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GRNNLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_reward_set.csv
Model: GRNN_1x3_RNN_acceptreject_symmetric_qp_no-punishment_2022_10_04_02_23_57_505538
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6863	Validation Loss: 0.6840
Epoch 500: 	Training Loss: 0.6221	Validation Loss: 0.6478
Epoch 1000: 	Training Loss: 0.6186	Validation Loss: 0.6484
Epoch 1500: 	Training Loss: 0.6187	Validation Loss: 0.6469
Epoch 2000: 	Training Loss: 0.6185	Validation Loss: 0.6474
Epoch 2500: 	Training Loss: 0.6173	Validation Loss: 0.6480
Epoch 3000: 	Training Loss: 0.6171	Validation Loss: 0.6482
Epoch 3500: 	Training Loss: 0.6172	Validation Loss: 0.6474
Epoch 4000: 	Training Loss: 0.6161	Validation Loss: 0.6453
Epoch 4500: 	Training Loss: 0.6157	Validation Loss: 0.6468
Epoch 5000: 	Training Loss: 0.6151	Validation Loss: 0.6467
Epoch 5500: 	Training Loss: 0.6140	Validation Loss: 0.6468
Epoch 6000: 	Training Loss: 0.6134	Validation Loss: 0.6437
Epoch 6500: 	Training Loss: 0.6223	Validation Loss: 0.6445
Epoch 7000: 	Training Loss: 0.6253	Validation Loss: 0.6549
Epoch 7500: 	Training Loss: 0.6125	Validation Loss: 0.6424
Epoch 8000: 	Training Loss: 0.6166	Validation Loss: 0.6457
Epoch 8500: 	Training Loss: 0.6471	Validation Loss: 0.6680
Epoch 9000: 	Training Loss: 0.6395	Validation Loss: 0.6495
Epoch 9500: 	Training Loss: 0.6306	Validation Loss: 0.6356
Epoch 10000: 	Training Loss: 0.6276	Validation Loss: 0.6569
Epoch 10500: 	Training Loss: 0.6354	Validation Loss: 0.6400
Epoch 11000: 	Training Loss: 0.6290	Validation Loss: 0.6407
Epoch 11500: 	Training Loss: 0.6454	Validation Loss: 0.6524
Early stopping at epoch 11978
Best validation loss: 0.6347
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6933	Validation Loss: 0.6919
Epoch 500: 	Training Loss: 0.6192	Validation Loss: 0.6375
Epoch 1000: 	Training Loss: 0.6198	Validation Loss: 0.6380
Epoch 1500: 	Training Loss: 0.6167	Validation Loss: 0.6379
Epoch 2000: 	Training Loss: 0.6157	Validation Loss: 0.6457
Epoch 2500: 	Training Loss: 0.6178	Validation Loss: 0.6437
Early stopping at epoch 2597
Best validation loss: 0.6282
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6924	Validation Loss: 0.6914
Epoch 500: 	Training Loss: 0.6173	Validation Loss: 0.6452
Epoch 1000: 	Training Loss: 0.6161	Validation Loss: 0.6439
Epoch 1500: 	Training Loss: 0.6151	Validation Loss: 0.6445
Epoch 2000: 	Training Loss: 0.6218	Validation Loss: 0.6566
Epoch 2500: 	Training Loss: 0.6161	Validation Loss: 0.6481
Early stopping at epoch 2591
Best validation loss: 0.6393
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6882	Validation Loss: 0.6871
Epoch 500: 	Training Loss: 0.6313	Validation Loss: 0.6323
Epoch 1000: 	Training Loss: 0.6196	Validation Loss: 0.6287
Epoch 1500: 	Training Loss: 0.6194	Validation Loss: 0.6311
Epoch 2000: 	Training Loss: 0.6195	Validation Loss: 0.6312
Epoch 2500: 	Training Loss: 0.6264	Validation Loss: 0.6314
Epoch 3000: 	Training Loss: 0.6183	Validation Loss: 0.6311
Epoch 3500: 	Training Loss: 0.6232	Validation Loss: 0.6322
Epoch 4000: 	Training Loss: 0.6178	Validation Loss: 0.6334
Early stopping at epoch 4319
Best validation loss: 0.6251
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6877	Validation Loss: 0.6860
Epoch 500: 	Training Loss: 0.6176	Validation Loss: 0.6479
Epoch 1000: 	Training Loss: 0.6161	Validation Loss: 0.6433
Epoch 1500: 	Training Loss: 0.6217	Validation Loss: 0.6502
Epoch 2000: 	Training Loss: 0.6211	Validation Loss: 0.6484
Epoch 2500: 	Training Loss: 0.6207	Validation Loss: 0.6453
Epoch 3000: 	Training Loss: 0.6214	Validation Loss: 0.6458
Epoch 3500: 	Training Loss: 0.6214	Validation Loss: 0.6495
Epoch 4000: 	Training Loss: 0.6216	Validation Loss: 0.6441
Epoch 4500: 	Training Loss: 0.6232	Validation Loss: 0.6517
Epoch 5000: 	Training Loss: 0.6217	Validation Loss: 0.6514
Epoch 5500: 	Training Loss: 0.6214	Validation Loss: 0.6461
Epoch 6000: 	Training Loss: 0.6186	Validation Loss: 0.6439
Early stopping at epoch 6423
Best validation loss: 0.6396
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/GRNN_1x3_RNN_acceptreject_symmetric_qp_no-punishment_2022_10_04_02_23_57_505538/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u18>
Subject: Job 126235138: <GRNNx3-0-1> in cluster <Janelia> Done

Job <GRNNx3-0-1> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Tue Oct  4 02:23:21 2022
Job was executed on host(s) <e10u18>, in queue <local>, as user <mohantas> in cluster <Janelia> at Tue Oct  4 02:23:38 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Tue Oct  4 02:23:38 2022
Terminated at Tue Oct  4 04:22:10 2022
Results reported at Tue Oct  4 04:22:10 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_rajagopalan.py --agent GRNN --num_reservoir 1 --reservoir_size 3 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric yes --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   14111.97 sec.
    Max Memory :                                 276 MB
    Average Memory :                             244.11 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               15084.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                14
    Run time :                                   7129 sec.
    Turnaround time :                            7129 sec.

The output (if any) is above this job summary.


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GRNNLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_reward_set.csv
Model: GRNN_1x3_RNN_acceptreject_symmetric_qp_no-punishment_2022_10_08_20_12_08_345403
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6748	Validation Loss: 0.6590
Epoch 500: 	Training Loss: 0.5953	Validation Loss: 0.6104
Epoch 1000: 	Training Loss: 0.5953	Validation Loss: 0.6094
Epoch 1500: 	Training Loss: 0.5957	Validation Loss: 0.6095
Epoch 2000: 	Training Loss: 0.5954	Validation Loss: 0.6107
Epoch 2500: 	Training Loss: 0.5946	Validation Loss: 0.6094
Early stopping at epoch 2620
Best validation loss: 0.6077
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6934	Validation Loss: 0.6860
Epoch 500: 	Training Loss: 0.5984	Validation Loss: 0.6018
Epoch 1000: 	Training Loss: 0.5983	Validation Loss: 0.6012
Epoch 1500: 	Training Loss: 0.5987	Validation Loss: 0.6003
Epoch 2000: 	Training Loss: 0.5966	Validation Loss: 0.5991
Epoch 2500: 	Training Loss: 0.6002	Validation Loss: 0.5973
Early stopping at epoch 2606
Best validation loss: 0.5967
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6856	Validation Loss: 0.6707
Epoch 500: 	Training Loss: 0.6006	Validation Loss: 0.5925
Epoch 1000: 	Training Loss: 0.5994	Validation Loss: 0.5918
Epoch 1500: 	Training Loss: 0.5987	Validation Loss: 0.5920
Epoch 2000: 	Training Loss: 0.5997	Validation Loss: 0.5922
Epoch 2500: 	Training Loss: 0.5988	Validation Loss: 0.5918
Epoch 3000: 	Training Loss: 0.5989	Validation Loss: 0.5932
Epoch 3500: 	Training Loss: 0.5989	Validation Loss: 0.5927
Epoch 4000: 	Training Loss: 0.5985	Validation Loss: 0.5951
Epoch 4500: 	Training Loss: 0.5984	Validation Loss: 0.5925
Epoch 5000: 	Training Loss: 0.6004	Validation Loss: 0.5931
Early stopping at epoch 5022
Best validation loss: 0.5908
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6888	Validation Loss: 0.6753
Epoch 500: 	Training Loss: 0.6008	Validation Loss: 0.5915
Epoch 1000: 	Training Loss: 0.5988	Validation Loss: 0.5915
Epoch 1500: 	Training Loss: 0.5992	Validation Loss: 0.5912
Epoch 2000: 	Training Loss: 0.6014	Validation Loss: 0.5931
Epoch 2500: 	Training Loss: 0.5983	Validation Loss: 0.5954
Early stopping at epoch 2992
Best validation loss: 0.5885
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6739	Validation Loss: 0.6688
Epoch 500: 	Training Loss: 0.5903	Validation Loss: 0.6338
Epoch 1000: 	Training Loss: 0.5901	Validation Loss: 0.6358
Epoch 1500: 	Training Loss: 0.5894	Validation Loss: 0.6289
Epoch 2000: 	Training Loss: 0.5897	Validation Loss: 0.6289
Epoch 2500: 	Training Loss: 0.5899	Validation Loss: 0.6309
Early stopping at epoch 2567
Best validation loss: 0.6255
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/GRNN_1x3_RNN_acceptreject_symmetric_qp_no-punishment_2022_10_08_20_12_08_345403/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u17>
Subject: Job 126381298: <GRNNx3-0-1> in cluster <Janelia> Done

Job <GRNNx3-0-1> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Sat Oct  8 20:11:30 2022
Job was executed on host(s) <e10u17>, in queue <local>, as user <mohantas> in cluster <Janelia> at Sat Oct  8 20:11:30 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Sat Oct  8 20:11:30 2022
Terminated at Sun Oct  9 01:12:05 2022
Results reported at Sun Oct  9 01:12:05 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_mohanta.py --agent GRNN --num_reservoir 1 --reservoir_size 3 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric yes --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   35380.01 sec.
    Max Memory :                                 319 MB
    Average Memory :                             253.12 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               15041.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                14
    Run time :                                   18038 sec.
    Turnaround time :                            18035 sec.

The output (if any) is above this job summary.

