
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GQLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_reward_set.csv
Model: GQNN_5-5_relu_acceptreject_asymmetric_qp_no-punishment_2022_10_04_02_25_55_496072
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6956	Validation Loss: 0.6937
Epoch 500: 	Training Loss: 0.6338	Validation Loss: 0.6258
Epoch 1000: 	Training Loss: 0.6320	Validation Loss: 0.6289
Epoch 1500: 	Training Loss: 0.6306	Validation Loss: 0.6267
Epoch 2000: 	Training Loss: 0.6363	Validation Loss: 0.6300
Epoch 2500: 	Training Loss: 0.6273	Validation Loss: 0.6584
Epoch 3000: 	Training Loss: 0.6311	Validation Loss: 0.6181
Epoch 3500: 	Training Loss: 0.6302	Validation Loss: 0.6232
Epoch 4000: 	Training Loss: 0.6428	Validation Loss: 0.6506
Epoch 4500: 	Training Loss: 0.6423	Validation Loss: 0.6309
Epoch 5000: 	Training Loss: 0.6663	Validation Loss: 0.6679
Epoch 5500: 	Training Loss: 0.6314	Validation Loss: 0.6240
Epoch 6000: 	Training Loss: 0.6328	Validation Loss: 0.6254
Early stopping at epoch 6171
Best validation loss: 0.6104
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6299	Validation Loss: 0.6155
Epoch 500: 	Training Loss: 0.6326	Validation Loss: 0.6232
Epoch 1000: 	Training Loss: 0.6346	Validation Loss: 0.6205
Epoch 1500: 	Training Loss: 0.6309	Validation Loss: 0.6270
Epoch 2000: 	Training Loss: 0.6330	Validation Loss: 0.6224
Epoch 2500: 	Training Loss: 0.6442	Validation Loss: 0.6381
Early stopping at epoch 2506
Best validation loss: 0.6136
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6317	Validation Loss: 0.6120
Epoch 500: 	Training Loss: 0.6277	Validation Loss: 0.6235
Epoch 1000: 	Training Loss: 0.6524	Validation Loss: 0.6601
Epoch 1500: 	Training Loss: 0.6610	Validation Loss: 0.6564
Epoch 2000: 	Training Loss: 0.6733	Validation Loss: 0.6643
Epoch 2500: 	Training Loss: 0.7289	Validation Loss: 0.7152
Early stopping at epoch 2728
Best validation loss: 0.6072
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6223	Validation Loss: 0.6480
Epoch 500: 	Training Loss: 0.6232	Validation Loss: 0.6612
Epoch 1000: 	Training Loss: 0.6890	Validation Loss: 0.6683
Epoch 1500: 	Training Loss: 0.6411	Validation Loss: 0.6704
Epoch 2000: 	Training Loss: 0.6659	Validation Loss: 0.7213
Epoch 2500: 	Training Loss: 0.6223	Validation Loss: 0.6614
Early stopping at epoch 2513
Best validation loss: 0.6453
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6391	Validation Loss: 0.5844
Epoch 500: 	Training Loss: 0.6352	Validation Loss: 0.5962
Epoch 1000: 	Training Loss: 0.6335	Validation Loss: 0.5959
Epoch 1500: 	Training Loss: 0.6335	Validation Loss: 0.6008
Epoch 2000: 	Training Loss: 0.6340	Validation Loss: 0.6036
Epoch 2500: 	Training Loss: 0.6314	Validation Loss: 0.5995
Early stopping at epoch 2500
Best validation loss: 0.5844
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/GQNN_5-5_relu_acceptreject_asymmetric_qp_no-punishment_2022_10_04_02_25_55_496072/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u15>
Subject: Job 126235983: <GQNNx5x5-0-0> in cluster <Janelia> Done

Job <GQNNx5x5-0-0> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Tue Oct  4 02:25:23 2022
Job was executed on host(s) <e10u15>, in queue <local>, as user <mohantas> in cluster <Janelia> at Tue Oct  4 02:25:24 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Tue Oct  4 02:25:24 2022
Terminated at Tue Oct  4 10:20:25 2022
Results reported at Tue Oct  4 10:20:25 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_rajagopalan.py --agent GQNN --hidden_state_sizes 5 5 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric no --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   56703.73 sec.
    Max Memory :                                 263 MB
    Average Memory :                             245.21 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               15097.00 MB
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                14
    Run time :                                   28500 sec.
    Turnaround time :                            28502 sec.

The output (if any) is above this job summary.


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GQLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_reward_set.csv
Model: GQNN_5-5_relu_acceptreject_asymmetric_qp_no-punishment_2022_10_08_20_15_30_796343
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6807	Validation Loss: 0.6715
Epoch 500: 	Training Loss: 0.6021	Validation Loss: 0.5907
Epoch 1000: 	Training Loss: 0.6184	Validation Loss: 0.6430
Epoch 1500: 	Training Loss: 0.6008	Validation Loss: 0.5935
Epoch 2000: 	Training Loss: 0.6026	Validation Loss: 0.5930
Epoch 2500: 	Training Loss: 0.6025	Validation Loss: 0.5928
Epoch 3000: 	Training Loss: 0.6058	Validation Loss: 0.5948
Epoch 3500: 	Training Loss: 0.6160	Validation Loss: 0.5959
Epoch 4000: 	Training Loss: 0.6007	Validation Loss: 0.5921
Epoch 4500: 	Training Loss: 0.5996	Validation Loss: 0.5912
Epoch 5000: 	Training Loss: 0.5988	Validation Loss: 0.5917
Epoch 5500: 	Training Loss: 0.5988	Validation Loss: 0.5923
Epoch 6000: 	Training Loss: 0.5998	Validation Loss: 0.5948
Epoch 6500: 	Training Loss: 0.6006	Validation Loss: 0.5904
Epoch 7000: 	Training Loss: 0.6049	Validation Loss: 0.5988
Early stopping at epoch 7210
Best validation loss: 0.5890
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6127	Validation Loss: 0.5593
Epoch 500: 	Training Loss: 0.6252	Validation Loss: 0.5639
Epoch 1000: 	Training Loss: 0.6124	Validation Loss: 0.6716
Epoch 1500: 	Training Loss: 0.6071	Validation Loss: 0.5635
Epoch 2000: 	Training Loss: 0.6068	Validation Loss: 0.5648
Epoch 2500: 	Training Loss: 0.6087	Validation Loss: 0.5627
Epoch 3000: 	Training Loss: 0.6072	Validation Loss: 0.5597
Epoch 3500: 	Training Loss: 0.6067	Validation Loss: 0.5600
Early stopping at epoch 3622
Best validation loss: 0.5582
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6033	Validation Loss: 0.5743
Epoch 500: 	Training Loss: 0.6030	Validation Loss: 0.5759
Epoch 1000: 	Training Loss: 0.6080	Validation Loss: 0.5757
Epoch 1500: 	Training Loss: 0.6039	Validation Loss: 0.5769
Epoch 2000: 	Training Loss: 0.6037	Validation Loss: 0.5760
Epoch 2500: 	Training Loss: 0.6046	Validation Loss: 0.5776
Early stopping at epoch 2722
Best validation loss: 0.5739
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6018	Validation Loss: 0.5804
Epoch 500: 	Training Loss: 0.6019	Validation Loss: 0.5816
Epoch 1000: 	Training Loss: 0.6012	Validation Loss: 0.5837
Epoch 1500: 	Training Loss: 0.6036	Validation Loss: 0.5842
Epoch 2000: 	Training Loss: 0.6056	Validation Loss: 0.5830
Epoch 2500: 	Training Loss: 0.6026	Validation Loss: 0.5833
Epoch 3000: 	Training Loss: 0.6036	Validation Loss: 0.5846
Epoch 3500: 	Training Loss: 0.6023	Validation Loss: 0.5815
Epoch 4000: 	Training Loss: 0.6017	Validation Loss: 0.5863
Epoch 4500: 	Training Loss: 0.6040	Validation Loss: 0.5883
Early stopping at epoch 4924
Best validation loss: 0.5793
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6018	Validation Loss: 0.5930
Epoch 500: 	Training Loss: 0.5997	Validation Loss: 0.5907
Epoch 1000: 	Training Loss: 0.6003	Validation Loss: 0.5917
Epoch 1500: 	Training Loss: 0.5993	Validation Loss: 0.5907
Epoch 2000: 	Training Loss: 0.5998	Validation Loss: 0.5916
Epoch 2500: 	Training Loss: 0.6003	Validation Loss: 0.5910
Epoch 3000: 	Training Loss: 0.5997	Validation Loss: 0.5917
Epoch 3500: 	Training Loss: 0.6090	Validation Loss: 0.5933
Epoch 4000: 	Training Loss: 0.5995	Validation Loss: 0.5918
Epoch 4500: 	Training Loss: 0.5993	Validation Loss: 0.5932
Epoch 5000: 	Training Loss: 0.6008	Validation Loss: 0.5910
Epoch 5500: 	Training Loss: 0.6166	Validation Loss: 0.5958
Epoch 6000: 	Training Loss: 0.6025	Validation Loss: 0.5909
Early stopping at epoch 6033
Best validation loss: 0.5889
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/GQNN_5-5_relu_acceptreject_asymmetric_qp_no-punishment_2022_10_08_20_15_30_796343/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u14>
Subject: Job 126381394: <GQNNx5x5-0-0> in cluster <Janelia> Done

Job <GQNNx5x5-0-0> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Sat Oct  8 20:15:23 2022
Job was executed on host(s) <e10u14>, in queue <local>, as user <mohantas> in cluster <Janelia> at Sat Oct  8 20:15:23 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Sat Oct  8 20:15:23 2022
Terminated at Mon Oct 10 01:21:32 2022
Results reported at Mon Oct 10 01:21:32 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_mohanta.py --agent GQNN --hidden_state_sizes 5 5 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric no --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   207942.48 sec.
    Max Memory :                                 368 MB
    Average Memory :                             268.60 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               14992.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                14
    Run time :                                   104769 sec.
    Turnaround time :                            104769 sec.

The output (if any) is above this job summary.

