
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GRNNLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_reward_set.csv
Model: GRNN_1x2_RNN_acceptreject_asymmetric_qp_no-punishment_2022_10_04_02_17_35_285667
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6971	Validation Loss: 0.6959
Epoch 500: 	Training Loss: 0.6403	Validation Loss: 0.6144
Epoch 1000: 	Training Loss: 0.6329	Validation Loss: 0.6091
Epoch 1500: 	Training Loss: 0.6321	Validation Loss: 0.6087
Epoch 2000: 	Training Loss: 0.6315	Validation Loss: 0.6076
Epoch 2500: 	Training Loss: 0.6315	Validation Loss: 0.6077
Epoch 3000: 	Training Loss: 0.6324	Validation Loss: 0.6069
Epoch 3500: 	Training Loss: 0.6314	Validation Loss: 0.6068
Epoch 4000: 	Training Loss: 0.6310	Validation Loss: 0.6068
Epoch 4500: 	Training Loss: 0.6311	Validation Loss: 0.6076
Early stopping at epoch 4948
Best validation loss: 0.6043
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6867	Validation Loss: 0.7004
Epoch 500: 	Training Loss: 0.6276	Validation Loss: 0.6496
Epoch 1000: 	Training Loss: 0.6258	Validation Loss: 0.6348
Epoch 1500: 	Training Loss: 0.6250	Validation Loss: 0.6316
Epoch 2000: 	Training Loss: 0.6253	Validation Loss: 0.6317
Epoch 2500: 	Training Loss: 0.6227	Validation Loss: 0.6310
Epoch 3000: 	Training Loss: 0.6234	Validation Loss: 0.6320
Epoch 3500: 	Training Loss: 0.6231	Validation Loss: 0.6325
Epoch 4000: 	Training Loss: 0.6226	Validation Loss: 0.6311
Early stopping at epoch 4099
Best validation loss: 0.6296
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6818	Validation Loss: 0.6811
Epoch 500: 	Training Loss: 0.6383	Validation Loss: 0.6221
Epoch 1000: 	Training Loss: 0.6258	Validation Loss: 0.6196
Epoch 1500: 	Training Loss: 0.6254	Validation Loss: 0.6215
Epoch 2000: 	Training Loss: 0.6298	Validation Loss: 0.6188
Epoch 2500: 	Training Loss: 0.6256	Validation Loss: 0.6188
Epoch 3000: 	Training Loss: 0.6267	Validation Loss: 0.6168
Epoch 3500: 	Training Loss: 0.6259	Validation Loss: 0.6173
Epoch 4000: 	Training Loss: 0.6277	Validation Loss: 0.6156
Early stopping at epoch 4128
Best validation loss: 0.6149
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6964	Validation Loss: 0.6905
Epoch 500: 	Training Loss: 0.6284	Validation Loss: 0.6352
Epoch 1000: 	Training Loss: 0.6274	Validation Loss: 0.6355
Epoch 1500: 	Training Loss: 0.6281	Validation Loss: 0.6369
Epoch 2000: 	Training Loss: 0.6286	Validation Loss: 0.6356
Epoch 2500: 	Training Loss: 0.6271	Validation Loss: 0.6367
Epoch 3000: 	Training Loss: 0.6293	Validation Loss: 0.6378
Epoch 3500: 	Training Loss: 0.6287	Validation Loss: 0.6357
Epoch 4000: 	Training Loss: 0.6273	Validation Loss: 0.6354
Epoch 4500: 	Training Loss: 0.6296	Validation Loss: 0.6362
Epoch 5000: 	Training Loss: 0.6300	Validation Loss: 0.6357
Early stopping at epoch 5381
Best validation loss: 0.6331
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.7166	Validation Loss: 0.7109
Epoch 500: 	Training Loss: 0.6297	Validation Loss: 0.6209
Epoch 1000: 	Training Loss: 0.6296	Validation Loss: 0.6212
Epoch 1500: 	Training Loss: 0.6290	Validation Loss: 0.6226
Epoch 2000: 	Training Loss: 0.6290	Validation Loss: 0.6247
Epoch 2500: 	Training Loss: 0.6285	Validation Loss: 0.6242
Early stopping at epoch 2872
Best validation loss: 0.6193
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/GRNN_1x2_RNN_acceptreject_asymmetric_qp_no-punishment_2022_10_04_02_17_35_285667/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u07>
Subject: Job 126232876: <GRNNx2-0-0> in cluster <Janelia> Done

Job <GRNNx2-0-0> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Tue Oct  4 02:17:01 2022
Job was executed on host(s) <e10u07>, in queue <local>, as user <mohantas> in cluster <Janelia> at Tue Oct  4 02:17:02 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Tue Oct  4 02:17:02 2022
Terminated at Tue Oct  4 04:06:41 2022
Results reported at Tue Oct  4 04:06:41 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_rajagopalan.py --agent GRNN --num_reservoir 1 --reservoir_size 2 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric no --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   13000.20 sec.
    Max Memory :                                 252 MB
    Average Memory :                             233.97 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               15108.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                14
    Run time :                                   6579 sec.
    Turnaround time :                            6580 sec.

The output (if any) is above this job summary.


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GRNNLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_reward_set.csv
Model: GRNN_1x2_RNN_acceptreject_asymmetric_qp_no-punishment_2022_10_08_20_14_37_282292
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6708	Validation Loss: 0.6623
Epoch 500: 	Training Loss: 0.5915	Validation Loss: 0.6256
Epoch 1000: 	Training Loss: 0.5925	Validation Loss: 0.6279
Epoch 1500: 	Training Loss: 0.5946	Validation Loss: 0.6259
Epoch 2000: 	Training Loss: 0.5911	Validation Loss: 0.6293
Epoch 2500: 	Training Loss: 0.5919	Validation Loss: 0.6258
Epoch 3000: 	Training Loss: 0.5908	Validation Loss: 0.6271
Epoch 3500: 	Training Loss: 0.5922	Validation Loss: 0.6266
Epoch 4000: 	Training Loss: 0.5919	Validation Loss: 0.6269
Early stopping at epoch 4497
Best validation loss: 0.6248
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6963	Validation Loss: 0.6916
Epoch 500: 	Training Loss: 0.6089	Validation Loss: 0.5694
Epoch 1000: 	Training Loss: 0.6087	Validation Loss: 0.5690
Epoch 1500: 	Training Loss: 0.6094	Validation Loss: 0.5678
Epoch 2000: 	Training Loss: 0.6094	Validation Loss: 0.5691
Epoch 2500: 	Training Loss: 0.6087	Validation Loss: 0.5683
Epoch 3000: 	Training Loss: 0.6093	Validation Loss: 0.5686
Epoch 3500: 	Training Loss: 0.6086	Validation Loss: 0.5680
Epoch 4000: 	Training Loss: 0.6099	Validation Loss: 0.5678
Epoch 4500: 	Training Loss: 0.6089	Validation Loss: 0.5677
Epoch 5000: 	Training Loss: 0.6092	Validation Loss: 0.5703
Epoch 5500: 	Training Loss: 0.6097	Validation Loss: 0.5681
Epoch 6000: 	Training Loss: 0.6092	Validation Loss: 0.5676
Epoch 6500: 	Training Loss: 0.6090	Validation Loss: 0.5683
Epoch 7000: 	Training Loss: 0.6094	Validation Loss: 0.5710
Epoch 7500: 	Training Loss: 0.6099	Validation Loss: 0.5714
Epoch 8000: 	Training Loss: 0.6087	Validation Loss: 0.5675
Early stopping at epoch 8208
Best validation loss: 0.5665
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6730	Validation Loss: 0.6686
Epoch 500: 	Training Loss: 0.5940	Validation Loss: 0.6282
Epoch 1000: 	Training Loss: 0.5937	Validation Loss: 0.6317
Epoch 1500: 	Training Loss: 0.5928	Validation Loss: 0.6282
Epoch 2000: 	Training Loss: 0.5930	Validation Loss: 0.6269
Epoch 2500: 	Training Loss: 0.5937	Validation Loss: 0.6283
Epoch 3000: 	Training Loss: 0.5935	Validation Loss: 0.6316
Early stopping at epoch 3202
Best validation loss: 0.6256
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6816	Validation Loss: 0.6770
Epoch 500: 	Training Loss: 0.5983	Validation Loss: 0.6059
Epoch 1000: 	Training Loss: 0.5965	Validation Loss: 0.6086
Epoch 1500: 	Training Loss: 0.5962	Validation Loss: 0.6070
Epoch 2000: 	Training Loss: 0.5970	Validation Loss: 0.6125
Epoch 2500: 	Training Loss: 0.5954	Validation Loss: 0.6053
Epoch 3000: 	Training Loss: 0.5964	Validation Loss: 0.6070
Epoch 3500: 	Training Loss: 0.5963	Validation Loss: 0.6059
Epoch 4000: 	Training Loss: 0.5986	Validation Loss: 0.6097
Epoch 4500: 	Training Loss: 0.5967	Validation Loss: 0.6062
Epoch 5000: 	Training Loss: 0.5968	Validation Loss: 0.6091
Epoch 5500: 	Training Loss: 0.5954	Validation Loss: 0.6063
Epoch 6000: 	Training Loss: 0.5968	Validation Loss: 0.6064
Epoch 6500: 	Training Loss: 0.5969	Validation Loss: 0.6075
Early stopping at epoch 6726
Best validation loss: 0.6050
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6984	Validation Loss: 0.6910
Epoch 500: 	Training Loss: 0.5932	Validation Loss: 0.6185
Epoch 1000: 	Training Loss: 0.5938	Validation Loss: 0.6202
Epoch 1500: 	Training Loss: 0.5928	Validation Loss: 0.6214
Epoch 2000: 	Training Loss: 0.5935	Validation Loss: 0.6176
Epoch 2500: 	Training Loss: 0.5934	Validation Loss: 0.6164
Early stopping at epoch 2544
Best validation loss: 0.6149
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/GRNN_1x2_RNN_acceptreject_asymmetric_qp_no-punishment_2022_10_08_20_14_37_282292/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u28>
Subject: Job 126381318: <GRNNx2-0-0> in cluster <Janelia> Done

Job <GRNNx2-0-0> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Sat Oct  8 20:14:29 2022
Job was executed on host(s) <e10u28>, in queue <local>, as user <mohantas> in cluster <Janelia> at Sat Oct  8 20:14:29 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Sat Oct  8 20:14:29 2022
Terminated at Sun Oct  9 01:48:33 2022
Results reported at Sun Oct  9 01:48:33 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_mohanta.py --agent GRNN --num_reservoir 1 --reservoir_size 2 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric no --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   39152.76 sec.
    Max Memory :                                 369 MB
    Average Memory :                             263.70 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               14991.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                14
    Run time :                                   20045 sec.
    Turnaround time :                            20044 sec.

The output (if any) is above this job summary.

