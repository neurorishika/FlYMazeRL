
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GQLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/rajagopalan2022/training_reward_set.csv
Model: GQNN_2-2_relu_acceptreject_asymmetric_qp_no-punishment_2022_10_04_02_25_43_650160
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6933	Validation Loss: 0.6925
Epoch 500: 	Training Loss: 0.6488	Validation Loss: 0.6466
Epoch 1000: 	Training Loss: 0.6482	Validation Loss: 0.6481
Epoch 1500: 	Training Loss: 0.6475	Validation Loss: 0.6477
Epoch 2000: 	Training Loss: 0.6471	Validation Loss: 0.6471
Epoch 2500: 	Training Loss: 0.6470	Validation Loss: 0.6386
Epoch 3000: 	Training Loss: 0.6461	Validation Loss: 0.6379
Epoch 3500: 	Training Loss: 0.6451	Validation Loss: 0.6406
Epoch 4000: 	Training Loss: 0.6469	Validation Loss: 0.6443
Epoch 4500: 	Training Loss: 0.6450	Validation Loss: 0.6385
Epoch 5000: 	Training Loss: 0.6453	Validation Loss: 0.6390
Early stopping at epoch 5016
Best validation loss: 0.6364
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6506	Validation Loss: 0.6233
Epoch 500: 	Training Loss: 0.6405	Validation Loss: 0.6217
Epoch 1000: 	Training Loss: 0.6375	Validation Loss: 0.6194
Epoch 1500: 	Training Loss: 0.6372	Validation Loss: 0.6185
Epoch 2000: 	Training Loss: 0.6381	Validation Loss: 0.6171
Epoch 2500: 	Training Loss: 0.6375	Validation Loss: 0.6172
Epoch 3000: 	Training Loss: 0.6359	Validation Loss: 0.6176
Epoch 3500: 	Training Loss: 0.6369	Validation Loss: 0.6176
Epoch 4000: 	Training Loss: 0.6357	Validation Loss: 0.6145
Epoch 4500: 	Training Loss: 0.6350	Validation Loss: 0.6151
Epoch 5000: 	Training Loss: 0.6365	Validation Loss: 0.6165
Epoch 5500: 	Training Loss: 0.6332	Validation Loss: 0.6189
Epoch 6000: 	Training Loss: 0.6358	Validation Loss: 0.6153
Epoch 6500: 	Training Loss: 0.6337	Validation Loss: 0.6146
Epoch 7000: 	Training Loss: 0.6337	Validation Loss: 0.6136
Epoch 7500: 	Training Loss: 0.6328	Validation Loss: 0.6148
Epoch 8000: 	Training Loss: 0.6337	Validation Loss: 0.6276
Epoch 8500: 	Training Loss: 0.6332	Validation Loss: 0.6195
Epoch 9000: 	Training Loss: 0.6336	Validation Loss: 0.6136
Epoch 9500: 	Training Loss: 0.6335	Validation Loss: 0.6126
Epoch 10000: 	Training Loss: 0.6364	Validation Loss: 0.6144
Epoch 10500: 	Training Loss: 0.6344	Validation Loss: 0.6160
Epoch 11000: 	Training Loss: 0.6337	Validation Loss: 0.6115
Epoch 11500: 	Training Loss: 0.6332	Validation Loss: 0.6116
Epoch 12000: 	Training Loss: 0.6326	Validation Loss: 0.6151
Epoch 12500: 	Training Loss: 0.6358	Validation Loss: 0.6128
Epoch 13000: 	Training Loss: 0.6353	Validation Loss: 0.6113
Epoch 13500: 	Training Loss: 0.6358	Validation Loss: 0.6215
Early stopping at epoch 13597
Best validation loss: 0.6080
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6363	Validation Loss: 0.6211
Epoch 500: 	Training Loss: 0.6300	Validation Loss: 0.6238
Epoch 1000: 	Training Loss: 0.6300	Validation Loss: 0.6261
Epoch 1500: 	Training Loss: 0.6312	Validation Loss: 0.6312
Epoch 2000: 	Training Loss: 0.6296	Validation Loss: 0.6243
Epoch 2500: 	Training Loss: 0.6304	Validation Loss: 0.6295
Epoch 3000: 	Training Loss: 0.6334	Validation Loss: 0.6233
Epoch 3500: 	Training Loss: 0.6316	Validation Loss: 0.6242
Early stopping at epoch 3633
Best validation loss: 0.6204
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6303	Validation Loss: 0.6263
Epoch 500: 	Training Loss: 0.6278	Validation Loss: 0.6292
Epoch 1000: 	Training Loss: 0.6281	Validation Loss: 0.6287
Epoch 1500: 	Training Loss: 0.6308	Validation Loss: 0.6298
Epoch 2000: 	Training Loss: 0.6269	Validation Loss: 0.6285
Epoch 2500: 	Training Loss: 0.6271	Validation Loss: 0.6306
Early stopping at epoch 2863
Best validation loss: 0.6259
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6344	Validation Loss: 0.6174
Epoch 500: 	Training Loss: 0.6303	Validation Loss: 0.6250
Epoch 1000: 	Training Loss: 0.6344	Validation Loss: 0.6226
Epoch 1500: 	Training Loss: 0.6299	Validation Loss: 0.6335
Epoch 2000: 	Training Loss: 0.6311	Validation Loss: 0.6262
Epoch 2500: 	Training Loss: 0.6331	Validation Loss: 0.6368
Early stopping at epoch 2502
Best validation loss: 0.6169
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/GQNN_2-2_relu_acceptreject_asymmetric_qp_no-punishment_2022_10_04_02_25_43_650160/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u21>
Subject: Job 126235979: <GQNNx2x2-1-0> in cluster <Janelia> Done

Job <GQNNx2x2-1-0> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Tue Oct  4 02:25:22 2022
Job was executed on host(s) <e10u21>, in queue <local>, as user <mohantas> in cluster <Janelia> at Tue Oct  4 02:25:22 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Tue Oct  4 02:25:22 2022
Terminated at Tue Oct  4 10:21:44 2022
Results reported at Tue Oct  4 10:21:44 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_rajagopalan.py --agent GQNN --hidden_state_sizes 2 2 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric no --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/rajagopalan2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   56910.66 sec.
    Max Memory :                                 279 MB
    Average Memory :                             244.08 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               15081.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                14
    Run time :                                   28584 sec.
    Turnaround time :                            28582 sec.

The output (if any) is above this job summary.


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
███████╗██╗     ██╗   ██╗███╗   ███╗ █████╗ ███████╗███████╗██████╗ ██╗     
██╔════╝██║     ╚██╗ ██╔╝████╗ ████║██╔══██╗╚══███╔╝██╔════╝██╔══██╗██║     
█████╗  ██║      ╚████╔╝ ██╔████╔██║███████║  ███╔╝ █████╗  ██████╔╝██║     
██╔══╝  ██║       ╚██╔╝  ██║╚██╔╝██║██╔══██║ ███╔╝  ██╔══╝  ██╔══██╗██║     
██║     ███████╗   ██║   ██║ ╚═╝ ██║██║  ██║███████╗███████╗██║  ██║███████╗
╚═╝     ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝╚══════╝
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Developed by:
    Rishika Mohanta, Research Technician, Turner Lab, Janelia Research Campus

Fitting agent: GQLearner
Loading data from:
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_choice_set.csv
/groups/turner/home/mohantas/project/FlYMazeRL//data/mohanta2022/training_reward_set.csv
Model: GQNN_2-2_relu_acceptreject_asymmetric_qp_no-punishment_2022_10_08_20_15_25_188983
Fitting model 1/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6892	Validation Loss: 0.6961
Epoch 500: 	Training Loss: 0.6102	Validation Loss: 0.5776
Epoch 1000: 	Training Loss: 0.6095	Validation Loss: 0.5714
Epoch 1500: 	Training Loss: 0.6110	Validation Loss: 0.5718
Epoch 2000: 	Training Loss: 0.6110	Validation Loss: 0.5749
Epoch 2500: 	Training Loss: 0.6093	Validation Loss: 0.5720
Epoch 3000: 	Training Loss: 0.6118	Validation Loss: 0.5702
Epoch 3500: 	Training Loss: 0.6090	Validation Loss: 0.5700
Epoch 4000: 	Training Loss: 0.6083	Validation Loss: 0.5735
Epoch 4500: 	Training Loss: 0.6087	Validation Loss: 0.5685
Epoch 5000: 	Training Loss: 0.6097	Validation Loss: 0.5693
Epoch 5500: 	Training Loss: 0.6092	Validation Loss: 0.5689
Epoch 6000: 	Training Loss: 0.6107	Validation Loss: 0.5702
Epoch 6500: 	Training Loss: 0.6082	Validation Loss: 0.5686
Epoch 7000: 	Training Loss: 0.6075	Validation Loss: 0.5741
Early stopping at epoch 7497
Best validation loss: 0.5679
Fitting model 2/5. Fold 1/1
Epoch 0: 	Training Loss: 0.5928	Validation Loss: 0.6290
Epoch 500: 	Training Loss: 0.5933	Validation Loss: 0.6314
Epoch 1000: 	Training Loss: 0.5920	Validation Loss: 0.6293
Epoch 1500: 	Training Loss: 0.5947	Validation Loss: 0.6422
Epoch 2000: 	Training Loss: 0.5924	Validation Loss: 0.6300
Epoch 2500: 	Training Loss: 0.5926	Validation Loss: 0.6308
Epoch 3000: 	Training Loss: 0.5936	Validation Loss: 0.6287
Early stopping at epoch 3068
Best validation loss: 0.6272
Fitting model 3/5. Fold 1/1
Epoch 0: 	Training Loss: 0.6062	Validation Loss: 0.5765
Epoch 500: 	Training Loss: 0.6058	Validation Loss: 0.5896
Epoch 1000: 	Training Loss: 0.6070	Validation Loss: 0.5830
Epoch 1500: 	Training Loss: 0.6059	Validation Loss: 0.5789
Epoch 2000: 	Training Loss: 0.6044	Validation Loss: 0.5824
Epoch 2500: 	Training Loss: 0.6055	Validation Loss: 0.5807
Epoch 3000: 	Training Loss: 0.6067	Validation Loss: 0.5765
Epoch 3500: 	Training Loss: 0.6055	Validation Loss: 0.5780
Epoch 4000: 	Training Loss: 0.6065	Validation Loss: 0.5779
Epoch 4500: 	Training Loss: 0.6057	Validation Loss: 0.5762
Early stopping at epoch 4922
Best validation loss: 0.5757
Fitting model 4/5. Fold 1/1
Epoch 0: 	Training Loss: 0.5950	Validation Loss: 0.6258
Epoch 500: 	Training Loss: 0.5945	Validation Loss: 0.6196
Epoch 1000: 	Training Loss: 0.5946	Validation Loss: 0.6345
Epoch 1500: 	Training Loss: 0.5958	Validation Loss: 0.6201
Epoch 2000: 	Training Loss: 0.5959	Validation Loss: 0.6280
Epoch 2500: 	Training Loss: 0.5941	Validation Loss: 0.6199
Epoch 3000: 	Training Loss: 0.5935	Validation Loss: 0.6193
Epoch 3500: 	Training Loss: 0.5939	Validation Loss: 0.6189
Epoch 4000: 	Training Loss: 0.5935	Validation Loss: 0.6294
Epoch 4500: 	Training Loss: 0.5939	Validation Loss: 0.6199
Epoch 5000: 	Training Loss: 0.5930	Validation Loss: 0.6206
Epoch 5500: 	Training Loss: 0.5933	Validation Loss: 0.6191
Epoch 6000: 	Training Loss: 0.5934	Validation Loss: 0.6203
Early stopping at epoch 6292
Best validation loss: 0.6184
Fitting model 5/5. Fold 1/1
Epoch 0: 	Training Loss: 0.5943	Validation Loss: 0.6219
Epoch 500: 	Training Loss: 0.5935	Validation Loss: 0.6228
Epoch 1000: 	Training Loss: 0.5938	Validation Loss: 0.6223
Epoch 1500: 	Training Loss: 0.5944	Validation Loss: 0.6230
Epoch 2000: 	Training Loss: 0.5960	Validation Loss: 0.6241
Epoch 2500: 	Training Loss: 0.5931	Validation Loss: 0.6237
Epoch 3000: 	Training Loss: 0.5932	Validation Loss: 0.6212
Epoch 3500: 	Training Loss: 0.5929	Validation Loss: 0.6225
Epoch 4000: 	Training Loss: 0.5942	Validation Loss: 0.6231
Epoch 4500: 	Training Loss: 0.5924	Validation Loss: 0.6216
Epoch 5000: 	Training Loss: 0.5947	Validation Loss: 0.6278
Epoch 5500: 	Training Loss: 0.5942	Validation Loss: 0.6241
Early stopping at epoch 5562
Best validation loss: 0.6207
Fitting is complete. The model fitting log is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/model_fitting_log.csv.
The model is available at /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/GQNN_2-2_relu_acceptreject_asymmetric_qp_no-punishment_2022_10_08_20_15_25_188983/.
Thank you for using flymazerl. Have a nice day :)

------------------------------------------------------------
Sender: LSF System <lsfadmin@e10u22>
Subject: Job 126381391: <GQNNx2x2-1-0> in cluster <Janelia> Done

Job <GQNNx2x2-1-0> was submitted from host <e05u15> by user <mohantas> in cluster <Janelia> at Sat Oct  8 20:15:21 2022
Job was executed on host(s) <e10u22>, in queue <local>, as user <mohantas> in cluster <Janelia> at Sat Oct  8 20:15:21 2022
</groups/turner/home/mohantas> was used as the home directory.
</groups/turner/home/mohantas/project/FlYMazeRL/flymazerl/scripts/bash> was used as the working directory.
Started at Sat Oct  8 20:15:21 2022
Terminated at Mon Oct 10 22:39:01 2022
Results reported at Mon Oct 10 22:39:01 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python ../nn_fitting_mohanta.py --agent GQNN --hidden_state_sizes 2 2 --n_folds 1 --n_ensemble 5 --early_stopping 2500 --symmetric no --save_path /groups/turner/turnerlab/Rishika/FlYMazeRL_Fits/nn/mohanta2022/ 
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   360593.44 sec.
    Max Memory :                                 388 MB
    Average Memory :                             277.90 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               14972.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                14
    Run time :                                   181417 sec.
    Turnaround time :                            181420 sec.

The output (if any) is above this job summary.

