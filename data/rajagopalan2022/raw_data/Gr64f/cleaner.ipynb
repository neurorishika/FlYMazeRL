{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\flymazerl\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py:226: MatReadWarning: Duplicate variable name \"None\" in stream - replacing previous with new\n",
      "Consider mio5.varmats_from_mat to split file into single variable files\n",
      "  matfile_dict = MR.get_variables(variable_names)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 20200617T114331_Y-Arena1_Cam0BR_V2_64f_Fly4 0.89 0.11 0.67\n",
      "1 20200617T142343_Y-Arena1_Cam0BR_V2_64f_Fly5 0.8 0.11 0.89\n",
      "2 20200620T132517_Y-Arena1_Cam0BR_V2_64f_Fly12 0.2 0.8 0.33\n",
      "Test Set 20200623T111945_Y-Arena1_Cam0BR_V2_64f_Fly15 142\n",
      "3 20200623T150218_Y-Arena1_Cam0BR_V2_64f_Fly16 0.5 0.89 0.2\n",
      "4 20200624T134618_Y-Arena1_Cam0BR_V2_64f_Fly18 0.2 0.8 0.89\n",
      "5 20210324T085138_Y-Arena1_Cam0BR_V2_gr64f_Fly1_round2 0.67 0.33 0.89\n",
      "6 20210324T144413_Y-Arena1_Cam0BR_V2_gr64f_Fly3_round2 0.5 0.67 0.2\n",
      "7 20210324T173338_Y-Arena1_Cam0BR_V2_gr64f_Fly4_round2 0.33 0.5 0.67\n",
      "Test Set 20210410T112739_Y-Arena1_Cam0BR_V2_gr64f_Fly5_round2 171\n",
      "8 20210410T182854_Y-Arena1_Cam0BR_V2_gr64f_Fly7_round2 0.2 0.67 0.8\n",
      "9 20210412T105011_Y-Arena1_Cam0BR_V2_gr64f_Fly8_round2 0.67 0.2 0.5\n",
      "10 20210413T132142_Y-Arena1_Cam0BR_V2_gr64f_Fly9_round2 0.5 0.89 0.33\n",
      "11 20200715T141812_Y-Arena1_Cam0BR_V2_64f_Fly25 0.055 0.445 0.25\n",
      "12 20200717T100639_Y-Arena1_Cam0BR_V2_64f_Fly30 0.445 0.055 0.333\n",
      "13 20200717T142109_Y-Arena1_Cam0BR_V2_64f_Fly31 0.4 0.055 0.445\n",
      "Test Set 20200720T102324_Y-Arena1_Cam0BR_V2_64f_Fly34 185\n",
      "14 20200721T141229_Y-Arena1_Cam0BR_V2_64f_Fly35 0.333 0.167 0.445\n",
      "15 20200722T101330_Y-Arena1_Cam0BR_V2_64f_Fly36 0.445 0.1 0.25\n",
      "16 20200723T142915_Y-Arena1_Cam0BR_V2_64f_Fly39 0.25 0.445 0.1\n",
      "17 20200804T141505_Y-Arena1_Cam0BR_V2_64f_Fly43 0.4 0.167 0.333\n"
     ]
    }
   ],
   "source": [
    "action_set = []\n",
    "reward_set = []\n",
    "\n",
    "test_action_set = []\n",
    "test_reward_set = []\n",
    "\n",
    "with open('log.csv','w') as f:\n",
    "    f.write('FlyID,Session Name,p1,p2,p3\\n')\n",
    "    index = 0\n",
    "    for i in os.listdir(\"Baited_Reward_V2_3Cont\"):\n",
    "        choice_mat = loadmat(os.path.join(\"Baited_Reward_V2_3Cont\", i, \"choice_order.mat\"))\n",
    "        reward_mat = loadmat(os.path.join(\"Baited_Reward_V2_3Cont\", i, \"reward_order.mat\"))\n",
    "        end = np.argmax(choice_mat[\"choice_order\"].T.flatten()==0)\n",
    "        if end == 0 or end > 200:\n",
    "            p1 = loadmat(os.path.join(\"Baited_Reward_V2_3Cont\", i, \"all_variables_contingency_1.mat\"))['x'].item()\n",
    "            p2 = loadmat(os.path.join(\"Baited_Reward_V2_3Cont\", i, \"all_variables_contingency_2.mat\"))['x'].item()\n",
    "            p3 = loadmat(os.path.join(\"Baited_Reward_V2_3Cont\", i, \"all_variables_contingency_3.mat\"))['x'].item()\n",
    "            choice = np.int32(choice_mat[\"choice_order\"].T.flatten()==2)[:200]\n",
    "            action_set.append(choice)\n",
    "            reward = np.int32(reward_mat['reward_order'].T.flatten()>0)[:200]\n",
    "            reward_set.append(reward)\n",
    "            f.write(str(index)+\",\"+i+\",\"+str(p1)+\",\"+str(p2)+\",\"+str(p3)+\"\\n\")\n",
    "            print(index,i,p1,p2,p3)\n",
    "            index += 1\n",
    "        else:\n",
    "            print(\"Test Set\",i,end)\n",
    "            choice = np.nan*np.ones(200)\n",
    "            choice[:end] = np.int32(choice_mat[\"choice_order\"].T.flatten()==2)[:end]\n",
    "            test_action_set.append(choice)\n",
    "            reward = np.nan*np.ones(200)\n",
    "            reward[:end] = np.int32(reward_mat['reward_order'].T.flatten()>0)[:end]\n",
    "            test_reward_set.append(reward)\n",
    "    \n",
    "    for i in os.listdir(\"Baited_Reward_V2_3Cont_Tp_0.5\"):\n",
    "        choice_mat = loadmat(os.path.join(\"Baited_Reward_V2_3Cont_Tp_0.5\", i, \"choice_order.mat\"))\n",
    "        reward_mat = loadmat(os.path.join(\"Baited_Reward_V2_3Cont_Tp_0.5\", i, \"reward_order.mat\"))\n",
    "        end = np.argmax(choice_mat[\"choice_order\"].T.flatten()==0)\n",
    "        if end == 0 or end > 200:\n",
    "            p1 = loadmat(os.path.join(\"Baited_Reward_V2_3Cont_Tp_0.5\", i, \"all_variables_contingency_1.mat\"))['x'].item()\n",
    "            p2 = loadmat(os.path.join(\"Baited_Reward_V2_3Cont_Tp_0.5\", i, \"all_variables_contingency_2.mat\"))['x'].item()\n",
    "            p3 = loadmat(os.path.join(\"Baited_Reward_V2_3Cont_Tp_0.5\", i, \"all_variables_contingency_3.mat\"))['x'].item()\n",
    "            choice = np.int32(choice_mat[\"choice_order\"].T.flatten()==2)[:200]\n",
    "            reward = np.int32(reward_mat['reward_order'].T.flatten()>0)[:200]\n",
    "            f.write(str(index)+\",\"+i+\",\"+str(p1)+\",\"+str(p2)+\",\"+str(p3)+\"\\n\")\n",
    "            print(index,i,p1,p2,p3)\n",
    "            index += 1\n",
    "        else:\n",
    "            print(\"Test Set\",i,end)\n",
    "            choice = np.nan*np.ones(200)\n",
    "            choice[:end] = np.int32(choice_mat[\"choice_order\"].T.flatten()==2)[:end]\n",
    "            test_action_set.append(choice)\n",
    "            reward = np.nan*np.ones(200)\n",
    "            reward[:end] = np.int32(reward_mat['reward_order'].T.flatten()>0)[:end]\n",
    "            test_reward_set.append(reward)\n",
    "\n",
    "action_set = np.array(action_set)\n",
    "reward_set = np.array(reward_set)\n",
    "test_action_set = np.array(test_action_set)\n",
    "test_reward_set = np.array(test_reward_set)\n",
    "\n",
    "np.savetxt(\"action_set.csv\", action_set, delimiter=\",\", fmt=\"%s\")\n",
    "np.savetxt(\"reward_set.csv\", reward_set, delimiter=\",\", fmt=\"%s\")\n",
    "np.savetxt(\"test_action_set.csv\", test_action_set, delimiter=\",\", fmt=\"%s\")\n",
    "np.savetxt(\"test_reward_set.csv\", test_reward_set, delimiter=\",\", fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.0625, 10.875 ,  9.0625])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def func(individual_costs, total_discount, total_tax):\n",
    "    individual_costs = np.array(individual_costs)\n",
    "    discount_per_individual = total_discount/np.sum(individual_costs) * individual_costs\n",
    "    tax_per_individual = total_tax*individual_costs/(np.sum(individual_costs))\n",
    "    return tax_per_individual + individual_costs - discount_per_individual\n",
    "\n",
    "func([10,12,10],6,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Generate the data using the function func\n",
    "n_people = 3\n",
    "n_samples = 10000\n",
    "individual_costs = np.random.uniform(0,1000,size=(n_people,n_samples))\n",
    "tax = np.random.uniform(0,1,size=(n_samples,))*np.sum(individual_costs,axis=0)\n",
    "discount = np.random.uniform(0,1,size=(n_samples,))*np.sum(individual_costs,axis=0)\n",
    "X = np.concatenate((individual_costs,tax.reshape(1,-1),discount.reshape(1,-1)),axis=0).T\n",
    "y = np.array([func(x[:3],x[3],x[4]) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\flymazerl\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\flymazerl\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:378: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return [base_lr * self.gamma ** (self.last_epoch // self.step_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: 873.5552368164062 Val Loss: 429.2549743652344\n",
      "Epoch: 1 Train Loss: 476.9923400878906 Val Loss: 352.9335632324219\n",
      "Epoch: 2 Train Loss: 423.7012634277344 Val Loss: 325.96990966796875\n",
      "Epoch: 3 Train Loss: 388.5700378417969 Val Loss: 296.8573303222656\n",
      "Epoch: 4 Train Loss: 348.9811096191406 Val Loss: 265.1082763671875\n",
      "Epoch: 5 Train Loss: 309.0766906738281 Val Loss: 231.36131286621094\n",
      "Epoch: 6 Train Loss: 235.27037048339844 Val Loss: 197.98658752441406\n",
      "Epoch: 7 Train Loss: 173.8957977294922 Val Loss: 165.455078125\n",
      "Epoch: 8 Train Loss: 122.87911224365234 Val Loss: 133.45050048828125\n",
      "Epoch: 9 Train Loss: 69.53870391845703 Val Loss: 99.34628295898438\n",
      "Epoch: 10 Train Loss: 70.00936126708984 Val Loss: 78.00382995605469\n",
      "Epoch: 11 Train Loss: 44.93580627441406 Val Loss: 65.25760650634766\n",
      "Epoch: 12 Train Loss: 80.40141296386719 Val Loss: 62.782081604003906\n",
      "Epoch: 13 Train Loss: 65.10164642333984 Val Loss: 60.85481262207031\n",
      "Epoch: 14 Train Loss: 39.512935638427734 Val Loss: 54.87034225463867\n",
      "Epoch: 15 Train Loss: 56.41745376586914 Val Loss: 60.684574127197266\n",
      "Epoch: 16 Train Loss: 26.428421020507812 Val Loss: 52.90586853027344\n",
      "Epoch: 17 Train Loss: 36.53001022338867 Val Loss: 50.08869552612305\n",
      "Epoch: 18 Train Loss: 36.28214645385742 Val Loss: 49.28080749511719\n",
      "Epoch: 19 Train Loss: 47.93359375 Val Loss: 43.582313537597656\n",
      "Epoch: 20 Train Loss: 37.79029846191406 Val Loss: 39.90456771850586\n",
      "Epoch: 21 Train Loss: 60.99440383911133 Val Loss: 47.4099006652832\n",
      "Epoch: 22 Train Loss: 35.710609436035156 Val Loss: 40.468109130859375\n",
      "Epoch: 23 Train Loss: 42.163421630859375 Val Loss: 40.72803497314453\n",
      "Epoch: 24 Train Loss: 34.26755142211914 Val Loss: 36.87905502319336\n",
      "Epoch: 25 Train Loss: 46.30317687988281 Val Loss: 60.23727035522461\n",
      "Epoch: 26 Train Loss: 23.189224243164062 Val Loss: 27.802854537963867\n",
      "Epoch: 27 Train Loss: 74.51165008544922 Val Loss: 91.93360137939453\n",
      "Epoch: 28 Train Loss: 24.518539428710938 Val Loss: 18.768199920654297\n",
      "Epoch: 29 Train Loss: 82.65413665771484 Val Loss: 64.98780059814453\n",
      "Epoch: 30 Train Loss: 39.20524978637695 Val Loss: 25.17181968688965\n",
      "Epoch: 31 Train Loss: 60.64004135131836 Val Loss: 60.687828063964844\n",
      "Epoch: 32 Train Loss: 39.47504425048828 Val Loss: 28.640625\n",
      "Epoch: 33 Train Loss: 83.2079849243164 Val Loss: 68.25706481933594\n",
      "Epoch: 34 Train Loss: 38.35617446899414 Val Loss: 29.032028198242188\n",
      "Epoch: 35 Train Loss: 57.51247024536133 Val Loss: 51.10365676879883\n",
      "Epoch: 36 Train Loss: 53.58929443359375 Val Loss: 30.94451141357422\n",
      "Epoch: 37 Train Loss: 31.574026107788086 Val Loss: 46.93045425415039\n",
      "Epoch: 38 Train Loss: 58.944820404052734 Val Loss: 36.98948287963867\n",
      "Epoch: 39 Train Loss: 61.27009963989258 Val Loss: 41.246726989746094\n",
      "Epoch: 40 Train Loss: 43.97365951538086 Val Loss: 28.425928115844727\n",
      "Epoch: 41 Train Loss: 58.75944137573242 Val Loss: 40.784385681152344\n",
      "Epoch: 42 Train Loss: 36.10996627807617 Val Loss: 24.150894165039062\n",
      "Epoch: 43 Train Loss: 29.46415138244629 Val Loss: 22.125778198242188\n",
      "Epoch: 44 Train Loss: 21.694528579711914 Val Loss: 28.449399948120117\n",
      "Epoch: 45 Train Loss: 20.541259765625 Val Loss: 24.66610336303711\n",
      "Epoch: 46 Train Loss: 31.589345932006836 Val Loss: 23.057706832885742\n",
      "Epoch: 47 Train Loss: 31.969797134399414 Val Loss: 21.73809242248535\n",
      "Epoch: 48 Train Loss: 23.878000259399414 Val Loss: 27.854084014892578\n",
      "Epoch: 49 Train Loss: 25.04506492614746 Val Loss: 29.63532066345215\n",
      "Epoch: 50 Train Loss: 23.23389434814453 Val Loss: 26.78643035888672\n",
      "Epoch: 51 Train Loss: 28.65380096435547 Val Loss: 27.28388023376465\n",
      "Epoch: 52 Train Loss: 26.615102767944336 Val Loss: 28.3372859954834\n",
      "Epoch: 53 Train Loss: 43.40971755981445 Val Loss: 25.631427764892578\n",
      "Epoch: 54 Train Loss: 26.561309814453125 Val Loss: 34.86945343017578\n",
      "Epoch: 55 Train Loss: 25.865819931030273 Val Loss: 30.170839309692383\n",
      "Epoch: 56 Train Loss: 27.285423278808594 Val Loss: 29.432024002075195\n",
      "Epoch: 57 Train Loss: 33.831111907958984 Val Loss: 45.319175720214844\n",
      "Epoch: 58 Train Loss: 35.559268951416016 Val Loss: 26.59170913696289\n",
      "Epoch: 59 Train Loss: 27.20981788635254 Val Loss: 47.906368255615234\n",
      "Epoch: 60 Train Loss: 33.747459411621094 Val Loss: 26.4744815826416\n",
      "Epoch: 61 Train Loss: 19.77092933654785 Val Loss: 44.1036376953125\n",
      "Epoch: 62 Train Loss: 33.524410247802734 Val Loss: 21.476856231689453\n",
      "Epoch: 63 Train Loss: 24.201385498046875 Val Loss: 39.12993240356445\n",
      "Epoch: 64 Train Loss: 27.289148330688477 Val Loss: 26.289588928222656\n",
      "Epoch: 65 Train Loss: 19.826242446899414 Val Loss: 49.635459899902344\n",
      "Epoch: 66 Train Loss: 29.077638626098633 Val Loss: 21.79674530029297\n",
      "Epoch: 67 Train Loss: 16.90092658996582 Val Loss: 44.601409912109375\n",
      "Epoch: 68 Train Loss: 20.722145080566406 Val Loss: 21.197460174560547\n",
      "Epoch: 69 Train Loss: 14.12451457977295 Val Loss: 48.104835510253906\n",
      "Epoch: 70 Train Loss: 16.62372589111328 Val Loss: 20.309152603149414\n",
      "Epoch: 71 Train Loss: 10.579140663146973 Val Loss: 46.411808013916016\n",
      "Epoch: 72 Train Loss: 14.219002723693848 Val Loss: 24.42230796813965\n",
      "Epoch: 73 Train Loss: 9.194440841674805 Val Loss: 53.84939956665039\n",
      "Epoch: 74 Train Loss: 19.01703453063965 Val Loss: 17.166658401489258\n",
      "Epoch: 75 Train Loss: 7.879543304443359 Val Loss: 73.65132141113281\n",
      "Epoch: 76 Train Loss: 22.95195960998535 Val Loss: 13.238266944885254\n",
      "Epoch: 77 Train Loss: 10.962623596191406 Val Loss: 56.75636291503906\n",
      "Epoch: 78 Train Loss: 10.523435592651367 Val Loss: 17.674537658691406\n",
      "Epoch: 79 Train Loss: 22.246070861816406 Val Loss: 59.01786804199219\n",
      "Epoch: 80 Train Loss: 23.10880470275879 Val Loss: 19.69147300720215\n",
      "Epoch: 81 Train Loss: 15.224347114562988 Val Loss: 37.81120300292969\n",
      "Epoch: 82 Train Loss: 14.793644905090332 Val Loss: 24.04632568359375\n",
      "Epoch: 83 Train Loss: 13.506244659423828 Val Loss: 26.95169448852539\n",
      "Epoch: 84 Train Loss: 13.221035957336426 Val Loss: 23.241365432739258\n",
      "Epoch: 85 Train Loss: 10.534031867980957 Val Loss: 29.339670181274414\n",
      "Epoch: 86 Train Loss: 14.49689769744873 Val Loss: 23.354785919189453\n",
      "Epoch: 87 Train Loss: 13.307113647460938 Val Loss: 25.620500564575195\n",
      "Epoch: 88 Train Loss: 15.85977840423584 Val Loss: 25.480443954467773\n",
      "Epoch: 89 Train Loss: 15.658726692199707 Val Loss: 25.195728302001953\n",
      "Epoch: 90 Train Loss: 16.07533836364746 Val Loss: 27.77434539794922\n",
      "Epoch: 91 Train Loss: 10.391056060791016 Val Loss: 21.851293563842773\n",
      "Epoch: 92 Train Loss: 18.245534896850586 Val Loss: 29.519062042236328\n",
      "Epoch: 93 Train Loss: 20.823171615600586 Val Loss: 31.48235511779785\n",
      "Epoch: 94 Train Loss: 20.212038040161133 Val Loss: 20.186124801635742\n",
      "Epoch: 95 Train Loss: 19.429397583007812 Val Loss: 25.114662170410156\n",
      "Epoch: 96 Train Loss: 16.9416561126709 Val Loss: 31.774442672729492\n",
      "Epoch: 97 Train Loss: 19.85678482055664 Val Loss: 20.134410858154297\n",
      "Epoch: 98 Train Loss: 22.06224822998047 Val Loss: 24.64956283569336\n",
      "Epoch: 99 Train Loss: 16.105775833129883 Val Loss: 24.767166137695312\n",
      "Epoch: 100 Train Loss: 17.85674285888672 Val Loss: 20.83452796936035\n",
      "Epoch: 101 Train Loss: 19.12373924255371 Val Loss: 19.087751388549805\n"
     ]
    }
   ],
   "source": [
    "# make a simple neural network to fit the data\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_input, hidden_layer_sizes, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.hidden_layers.append(nn.Linear(n_input, hidden_layer_sizes[0]))\n",
    "        for i in range(len(hidden_layer_sizes)-1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_layer_sizes[i], hidden_layer_sizes[i+1]))\n",
    "        self.output_layer = nn.Linear(hidden_layer_sizes[-1], n_output)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = F.relu(layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# Perform train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# make validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model using early stopping and lr scheduler\n",
    "network = Net(n_input=n_people+2, hidden_layer_sizes=[100,100,100], n_output=n_people)\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "criterion = nn.MSELoss()\n",
    "n_epochs = 10000\n",
    "patience = 100\n",
    "best_loss = np.inf\n",
    "best_model = None\n",
    "for epoch in range(n_epochs):\n",
    "    # Train the model\n",
    "    network.train()\n",
    "    for i, x in enumerate(X_train):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = network(torch.Tensor(x).float())\n",
    "        loss = criterion(y_pred, torch.Tensor(y_train[i]).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Validate the model\n",
    "    network.eval()\n",
    "    val_loss = 0\n",
    "    for i, x in enumerate(X_val):\n",
    "        y_pred = network(torch.Tensor(x).float())\n",
    "        val_loss += criterion(y_pred, torch.Tensor(y_val[i]).float())\n",
    "    val_loss /= len(X_val)\n",
    "    scheduler.step(val_loss)\n",
    "    print(\"Epoch:\", epoch, \"Train Loss:\", loss.item(), \"Val Loss:\", val_loss.item())\n",
    "    # Check if the model is the best\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model = network\n",
    "    # Check if the model is overfitting\n",
    "    if epoch > patience:\n",
    "        if val_loss > best_loss:\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1bc6635aead129ae5e44b7477cd2f864a11fdcb46f1249fa60a026af62e8401f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('flymazerl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
